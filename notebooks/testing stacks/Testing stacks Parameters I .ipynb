{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing u- and cstacks Parameters in the stacks pipeline\n",
    "\n",
    "\n",
    "\n",
    "Documentation of all testing of u- and cstacks parameters, using a subset of the Lane 1 samples' sequencing data. Runs from ustacks --> cstacks ---> sstacks ---> populations ---> Bowtie and BLAST screening. ([Within visual of workflow](https://github.com/mfisher5/mf-fish546-2016/blob/master/Diagrams/PopGen_Workflow.md), I am working within the third main step, \"building a reference genome\"). The *end goal* is to obtain quantitative measurments, such as # loci retained and heterozygosity, that can be used to find a balance between (1) parameters that are stringent enough to filter out poor quality sequence data, and (2) parameters that maximize the number of sequences that can be used in downstream data analyses of population structure. \n",
    "\n",
    "**ustacks:** align short-read seqeunces in matching stacks, compare the stacks to form a set of loci, detect SNPs at the loci for each individual\n",
    "\n",
    "**cstacks:** create a set of consensus loci and construct a catalog\n",
    "\n",
    "\n",
    "Research / notes on which parameters being tested and why [here](http://www.evernote.com/l/AooSJ1LxmrdPm6mvDDMxYXQ2DITT4PcwVaY/)\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**!!!!! THIS IS IMPORTANT: For organizational purposes, all of the \"testing stacks parameters\" related files were moved into a folder WITHIN my repo. In order to run the code directly from this notebook, you need to EXTRACT ALL FILES from that folder in the repo INTO THE ROOT DIRECTORY. ie., move the folder `mf-fish546-PCod/StacksTesting/UCstacksL1` --> `mf-fish546-PCod/UCstacksL1`. !!!!! ** This may seem unneccessarily complicated, but it was the only way to make the repo readable / make the more recent, important files easier to find and access. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Running ustacks\n",
    "### 10/18/2016\n",
    "The following code was run in the command line of a linux VM. \n",
    "\n",
    "**Base Script**\n",
    "\n",
    "This has already been run for the Lane 1 data analysis (see [Lane 1 full stacks pipeline nb](https://github.com/mfisher5/mf-fish546-PCod/blob/master/notebooks/Lane1data_full%20stacks%20pipeline.ipynb) using the parameter values: **-m 5 -M 3**\n",
    "\n",
    "The base ustacks script was run using ALL samples, in file `L1_mv_barcodesTOsampleTEST.txt`\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**other ustacks scripts**\n",
    "\n",
    "--> I first created a new file, `L1_mv_barcodesTOsampleTEST.txt`, that I put in the scripts folder. This has all of the samples EXCLUDING Yellow Sea samples. I excluded Yellow Sea samples because in this lane there are only 3; in most of the following runs of cstacks (all except base_batch12) I need at least 10 individuals per population. This is explained on the Evernote page above. \n",
    "<br>\n",
    "\n",
    "\n",
    "**stacks_m3**\n",
    "\n",
    "Went into radtags_ustacks_genShellPE.py and made the following parameter changes: **-m 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## From within the DataAnalysis (github = mf-fish546-PCod) folder ##\n",
    "cd scripts\n",
    "!python radtags_ustacks_genShellPE.py L1_mv_barcodesTOsampleTEST.txt\n",
    "mv new_ustacks_shell ustacks_m3_shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stacks_m10**\n",
    "\n",
    "Went into radtags_ustacks_genShellPE.py and made the following parameter changes: **-m 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## From within the DataAnalysis (github = mf-fish546-PCod) folder ##\n",
    "cd scripts\n",
    "!python radtags_ustacks_genShellPE.py L1_mv_barcodesTOsampleTEST.txt\n",
    "mv new_ustacks_shell ustacks_m10_shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stacks_M2**\n",
    "\n",
    "Went into radtags_ustacks_genShellPE.py and made the following parameter changes: **-m 5 -M 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## From within the DataAnalysis (github = mf-fish546-PCod) folder ##\n",
    "cd scripts\n",
    "!python radtags_ustacks_genShellPE.py L1_mv_barcodesTOsampleTEST.txt\n",
    "mv new_ustacks_shell ustacks_M2_shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all of the shell scripts had been generated, I moved them up one directory (github = mf-fish546-PCod). \n",
    "\n",
    "I then ran the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cd ../ \n",
    "#modify shell scripts so they run as a shell in the command line\n",
    "!chmod +x ustacks_base_shell\n",
    "!chmod +x ustacks_m3_shell\n",
    "!chmod +x ustacks_m10_shell\n",
    "!chmod +x ustacks_M2_shell\n",
    "\n",
    "#Run each of the shell scripts\n",
    "!./ ustacks_m3_shell\n",
    "!./ ustacks_m10_shell\n",
    "!./ ustacks_M2_shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should now be 4 files per individual in each of the 4 stacks folders (stacks_base, stacks_m3, stacks_m10, stacks_M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running cstacks\n",
    "### 10/21/2016\n",
    "\n",
    "\n",
    "All of the batches run below used 10 individuals per population to build the catalog. I also produced a batch that used only 12 individuals per population to build the catalog (off of the **Base** script). To see this, look at the [Lane 1 full stacks pipeline nb](https://www.evernote.com/Home.action#n=781efd9e-7cc2-4a6a-b413-150e8db65544&ses=4&sh=2&sds=5&). \n",
    "\n",
    "\n",
    "The following code was run in the command line of a linux VM. See 10/21-22 notes for attempts at running the shell / cstacks code within Jupyter. \n",
    "\n",
    "\n",
    "The parameters varied in cstacks were the # of individuals to make the catalog, and **-n**.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**stacks_base**\n",
    "\n",
    "\n",
    "*batch_12:* 12 individuals per pop for catalog, -n 3. To see this, look at the [Lane 1 full stacks pipeline nb](https://www.evernote.com/Home.action#n=781efd9e-7cc2-4a6a-b413-150e8db65544&ses=4&sh=2&sds=5&). \n",
    "\n",
    "*batch_300:* 30 individuals, -n 0\n",
    "\n",
    "*batch_303:* 30 individuals, -n 3\n",
    "\n",
    "\n",
    "**stacks_m3**\n",
    "\n",
    "*batch_330:* 30 individuals, -n 3\n",
    "\n",
    "\n",
    "**stacks_m10**\n",
    "\n",
    "*batch_1030:* 30 individuals, -n 3\n",
    "\n",
    "\n",
    "**stacks_M2**\n",
    "\n",
    "*batch_230:* 30 individuals, -n 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From within the DataAnalysis (github = mf-fish546-2016) folder\n",
    "cd scripts\n",
    "!python cstacks_generateCode_10-21.py samples_for_cstacks.txt #generate shell script\n",
    "mv new_cstacks_code cstacks_Shell_10-21 #rename shell script to be more informative\n",
    "!chmod +x cstacks_Shell_10-21\n",
    "#moved shell script from scripts/ to UCstacksL1/\n",
    "cd ../UCstacksL1\n",
    "!./cstacks_Shell_10-21 #run the shell script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran this code over the weekend. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10/25/2016\n",
    "\n",
    "Code run over the weekend expressed one error, where it aborted the run of the 4th cstacks code (batch 330, testing ustacks **-m 3**).\n",
    "Error was due to a lack of memory in the VM. Increased memory, and re-ran batch 330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cstacks -b 330 \\\n",
    "-s stacks_m3/GE011215_07.1 -s stacks_m3/GE012315_06.1 -s stacks_m3/GE012315_11.1 -s stacks_m3/GE012315_03.1 -s stacks_m3/GE011215_09.1 -s stacks_m3/GE011215_16.1 -s stacks_m3/GE011215_01.1 -s stacks_m3/GE011215_30.1 -s stacks_m3/GE011215_15.1 -s stacks_m3/GE011215_14.1 -s stacks_m3/NA021015_10.1 -s stacks_m3/NA021015_17.1 -s stacks_m3/NA021015_21.1 -s stacks_m3/NA021015_02.1 -s stacks_m3/NA021015_09.1 -s stacks_m3/NA021015_22.1 -s stacks_m3/NA021015_14.1 -s stacks_m3/NA021015_06.1 -s stacks_m3/NA021015_13.1 -s stacks_m3/NA021015_16.1 -s stacks_m3/PO020515_03.1 -s stacks_m3/PO010715_10.1 -s stacks_m3/PO020515_14.1 -s stacks_m3/PO010715_11.1 -s stacks_m3/PO020515_05.1 -s stacks_m3/PO020515_17.1 -s stacks_m3/PO020515_15.1 -s stacks_m3/PO010715_17.1 -s stacks_m3/PO020515_10.1 -s stacks_m3/PO020515_08.1 -o stacks_m3 \\\n",
    "-n 3 -p 6 2> cstacks_out_b330.stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:**\n",
    "\n",
    "within each of the `stacks_` folders in `UCstacksL1`, I have a set of 3 files for each batch run. For example, in the `stacks_base` folder: \n",
    "\n",
    "(1) batch_12.catalog.[alleles,snps,tags].tsv.gz\n",
    "(2) batch_300.catalog.[alleles,snps,tags].tsv.gz\n",
    "(3) batch_303.catalog.[alleles,snps,tags].tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Running sstacks\n",
    "\n",
    "### 10/26/2016\n",
    "\n",
    "**sstacks:** sets of stacks produced for each individual using \"ustacks\" are searched against the catalogs produced above in \"cstacks\"\n",
    "\n",
    "*NOT TESTING PARAMETERS FOR SSTACKS*\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**(1)** stacks_base / batch12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/mf-fish546-2016/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Changed the parameters in the `sstacks_generateCode.py` to: *\n",
    "<br>\n",
    "<br>\n",
    "-b 12 \n",
    "\n",
    "-c stacks_base/batch_12\n",
    "\n",
    "-o sstacks_batch12\n",
    "\n",
    "*Since batch12 has Yellow Sea samples included, use ALL L1 samples*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys \r\n",
      "myfile = open(sys.argv[1], \"r\")\t#open the file with your list of samples to use in cstacks\r\n",
      "newfile = open(\"sstacks_batch12_shell\", \"w\")\t#create a new file where the ustacks code will go\r\n",
      "newfile.write(\"#/bin/bash\" + \"\\n\")\r\n",
      "filestring = \"sstacks -b 12 -c stacks_base/batch_12 \" #choose batch ID and catalog files\r\n",
      "for line in myfile: \t\t\t#for each line in the barcode file\r\n",
      "\tlinelist=line.strip().split()\t\r\n",
      "\tsamplefile = linelist[2] + \".1\"\t\t#creates name of sample input file based on sample list in myfile\r\n",
      "\tnewstring = \"-s stacks_base/\" + samplefile + \" \"\t#creates a new -s entry for that sample input file\r\n",
      "\tfilestring += newstring\t\t\t# appends new -s string to \"filestring\"\r\n",
      "filestring += \"-o sstacks_batch12 -p 6 2>> sstacks_out_b12\"   #choose output folder and processor # and std.err out file\r\n",
      "newfile.write(filestring)\t\t#write entire string to the new file\r\n",
      "myfile.close()\r\n",
      "newfile.close()\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 14 sstacks_generateCode.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python sstacks_generateCode.py L1_mv_barcodesTOsample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mv sstacks_batch12_shell ../UCstacksL1/sstacks_batch12_shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1\n"
     ]
    }
   ],
   "source": [
    "cd ../UCstacksL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir sstacks_batch12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x sstacks_batch12_shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!./sstacks_batch12_shell #run in command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**(2)** stacks_base / batch303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Changed the parameters in the `sstacks_generateCode.py` to: *\n",
    "<br>\n",
    "<br>\n",
    "-b 303 \n",
    "\n",
    "-c stacks_base/batch_303\n",
    "\n",
    "-o sstacks_batch303\n",
    "\n",
    "*Since batch303 doesn not have Yellow Sea samples included, use only \"test\" set of L1 samples*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python sstacks_generateCode.py L1_mv_barcodesTOsampleTEST.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys \r\n",
      "myfile = open(sys.argv[1], \"r\")\t#open the file with your list of samples to use in cstacks\r\n",
      "newfile = open(\"sstacks_batch300_shell\", \"w\")\t#create a new file where the ustacks code will go\r\n",
      "newfile.write(\"#/bin/bash\" + \"\\n\")\r\n",
      "filestring = \"sstacks -b 300 -c stacks_base/batch_300 \" #choose batch ID and catalog files\r\n",
      "for line in myfile: \t\t\t#for each line in the barcode file\r\n",
      "\tlinelist=line.strip().split()\t\r\n",
      "\tsamplefile = linelist[2] + \".1\"\t\t#creates name of sample input file based on sample list in myfile\r\n",
      "\tnewstring = \"-s stacks_base/\" + samplefile + \" \"\t#creates a new -s entry for that sample input file\r\n",
      "\tfilestring += newstring\t\t\t# appends new -s string to \"filestring\"\r\n",
      "filestring += \"-o sstacks_batch300 -p 6 2>> sstacks_out_b300\"   #choose output folder and processor # and std.err out file\r\n",
      "newfile.write(filestring)\t\t#write entire string to the new file\r\n",
      "myfile.close()\r\n",
      "newfile.close()\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 14 sstacks_generateCode.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mv sstacks_batch300_shell ../UCstacksL1/sstacks_batch300_shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1\n"
     ]
    }
   ],
   "source": [
    "cd ../UCstacksL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir sstacks_batch300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x sstacks_batch12_shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!./sstacks_batch300_shell #run in command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "**(3,4,5,6)** Shell script for remaining batches\n",
    "\n",
    "*batch_303* \n",
    "\n",
    "*batch_330* \n",
    "\n",
    "*batch_1030* \n",
    "\n",
    "*batch_230* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys \r\n",
      "newfile = open(\"sstacks_shell_10-26\", \"w\")\t#create a new file where the ustacks code will go\r\n",
      "newfile.write(\"#!/bin/bash\" + \"\\n\")\r\n",
      "\r\n",
      "##first batch: 303\r\n",
      "myfile = open(sys.argv[1], \"r\")\t#open the file with your list of samples to use in cstacks\r\n",
      "filestring = \"sstacks -b 303 -c stacks_base/batch_303 \" #choose batch ID and catalog files\r\n",
      "for line in myfile: \t\t\t#for each line in the barcode file\r\n",
      "\tlinelist=line.strip().split()\t\r\n",
      "\tsamplefile = linelist[2] + \".1\"\t\t#creates name of sample input file based on sample list in myfile\r\n",
      "\tnewstring = \"-s stacks_base/\" + samplefile + \" \"\t#creates a new -s entry for that sample input file\r\n",
      "\tfilestring += newstring\t\t\t# appends new -s string to \"filestring\"\r\n",
      "myfile.close()\r\n",
      "filestring += \"-o sstacks_batch303 -p 6 2>> sstacks_out_b303\"   #choose output folder and processor # and std.err out file\r\n",
      "newfile.write(filestring + \"\\n\\n\\n\")\t\t#write entire string to the new file\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "##next batch: 330\r\n",
      "myfile = open(sys.argv[1], \"r\")\t#open the file with your list of samples to use in cstacks\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 20 sstacks_generateShell10-26.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python sstacks_generateShell10-26.py L1_mv_barcodesTOsampleTEST.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mv sstacks_shell_10-26 ../UCstacksL1/sstacks_shell_10-26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1\n"
     ]
    }
   ],
   "source": [
    "cd ../UCstacksL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir sstacks_batch303\n",
    "!mkdir sstacks_batch330\n",
    "!mkdir sstacks_batch1030\n",
    "!mkdir sstacks_batch230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x sstacks_shell_10-26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "./sstacks_shell_10-26 #ran in command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize file structure\n",
    "\n",
    "Move each output (stderr) file into the appropriate directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/mf-fish546-2016/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1\n"
     ]
    }
   ],
   "source": [
    "cd UCstacksL1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mcstacks_out4.stderr\u001b[0m*       \u001b[34;42msstacks_batch12\u001b[0m/         \u001b[01;32msstacks_out_b230\u001b[0m*\r\n",
      "\u001b[01;32mcstacks_out_b1030.stderr\u001b[0m*  \u001b[01;32msstacks_batch12_shell\u001b[0m*   \u001b[01;32msstacks_out_b300\u001b[0m*\r\n",
      "\u001b[01;32mcstacks_out_b230.stderr\u001b[0m*   \u001b[34;42msstacks_batch230\u001b[0m/        \u001b[01;32msstacks_out_b303\u001b[0m*\r\n",
      "\u001b[01;32mcstacks_out_b300.stderr\u001b[0m*   \u001b[34;42msstacks_batch300\u001b[0m/        \u001b[01;32msstacks_out_b330\u001b[0m*\r\n",
      "\u001b[01;32mcstacks_out_b303.stderr\u001b[0m*   \u001b[01;32msstacks_batch300_shell\u001b[0m*  \u001b[01;32msstacks_shell_10-26\u001b[0m*\r\n",
      "\u001b[01;32mcstacks_out_b330.stderr\u001b[0m*   \u001b[34;42msstacks_batch303\u001b[0m/        \u001b[34;42mstacks_base\u001b[0m/\r\n",
      "\u001b[01;32mcstacksOut_baseBatch12\u001b[0m*    \u001b[34;42msstacks_batch330\u001b[0m/        \u001b[34;42mstacks_m10\u001b[0m/\r\n",
      "\u001b[01;32mcstacks_Shell_10-21\u001b[0m*       \u001b[01;32msstacks_out_b1030\u001b[0m*       \u001b[34;42mstacks_M2\u001b[0m/\r\n",
      "\u001b[34;42msstacks_batch1030\u001b[0m/         \u001b[01;32msstacks_out_b12\u001b[0m*         \u001b[34;42mstacks_m3\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mv sstacks_out_b12 sstacks_batch12/sstacks_out_b12\n",
    "!mv sstacks_out_b300 sstacks_batch300/sstacks_out_b300\n",
    "!mv sstacks_out_b303 sstacks_batch303/sstacks_out_b303\n",
    "!mv sstacks_out_b330 sstacks_batch330/sstacks_out_b330\n",
    "!mv sstacks_out_b230 sstacks_batch230/sstacks_out_b230\n",
    "!mv sstacks_out_b1030 sstacks_batch1030/sstacks_out_b1030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat 'cstacks_Out_baseBatch12.stderr': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!mv cstacks_Out_baseBatch12.stderr stacks_base/cstacks_Out_baseBatch12.stderr\n",
    "!mv cstacks_out_b300.stderr stacks_base/cstacks_out_b300.stderr\n",
    "!mv cstacks_out_b303.stderr stacks_base/cstacks_out_b303.stderr\n",
    "!mv cstacks_out_b330.stderr stacks_m3/cstacks_out_b330.stderr\n",
    "!mv cstacks_out_b230.stderr stacks_M2/cstacks_out_b230.stderr\n",
    "!mv cstacks_out_b1030.stderr stacks_m10/cstacks_out_b1030.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mv cstacksOut_baseBatch12 stacks_base/cstacksOut_baseBatch12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Assessing differences between batches\n",
    "\n",
    "### 10/26/2016\n",
    "\n",
    "I need to output quantitative measurements that will show how the different parameters in each batch are going to effect my data. \n",
    "\n",
    "\n",
    "A full explanation of the thought process / research leading to the steps below can be found at the top of [this Evernote notebook](http://www.evernote.com/l/AoqT8ohibn9Dvafdz-TxhHAPav412kkvMZc/)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### (1) Run stacks program `populations`.\n",
    "Outputs: # loci before screening, observed heterozygosity per individual, files for BOWTIE and BLAST\n",
    "\n",
    "*Note:* `populations` requires both sstacks and ustacks files for a specific batch to be in the same folder. Since I have multiple sstacks batches from the same stacks_base folder with the ustacks files, I am going to make a copy of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/mf-fish546-2016/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1\n"
     ]
    }
   ],
   "source": [
    "cd UCstacksL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mcstacks_out4.stderr\u001b[0m*    \u001b[34;42msstacks_batch230\u001b[0m/        \u001b[01;32msstacks_shell_10-26\u001b[0m*\r\n",
      "\u001b[01;32mcstacks_Shell_10-21\u001b[0m*    \u001b[34;42msstacks_batch300\u001b[0m/        \u001b[34;42mstacks_base\u001b[0m/\r\n",
      "\u001b[34;42msstacks_batch1030\u001b[0m/      \u001b[01;32msstacks_batch300_shell\u001b[0m*  \u001b[34;42mstacks_m10\u001b[0m/\r\n",
      "\u001b[34;42msstacks_batch12\u001b[0m/        \u001b[34;42msstacks_batch303\u001b[0m/        \u001b[34;42mstacks_M2\u001b[0m/\r\n",
      "\u001b[01;32msstacks_batch12_shell\u001b[0m*  \u001b[34;42msstacks_batch330\u001b[0m/        \u001b[34;42mstacks_m3\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make new folder to hold sstacks and ustacks files for a certain run of populations when there is more than one batch per /stacks/ folder\n",
    "mkdir populations_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#copied all of the ustacks files from stacks_base into populations_temp directory\n",
    "#copied all of the batch_12.catalog files from stacks_base into populations_temp\n",
    "#copied all of the files from sstacks_batch12 in populations_temp directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i need to run the `populations` program. The data filtering options during these runs of `populations` will NOT be stringent, since I'm looking to compare between batches rather than create my actual reference genome. \n",
    "\n",
    "I need to create a \"Populations Map\" text file, a list of: sampleID (minus file extensions) <tab> population name\n",
    "\n",
    "I created this using a python script, written for any number of populations. However, the sample IDs MUST begin with the first two letters of the associated population name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populations:  ['Geoje', 'Pohang', 'Namhae', 'YellowSea']\r\n",
      "Population Map generated with  60 samples\r\n"
     ]
    }
   ],
   "source": [
    "!python genPopMap.py L1_mv_barcodesTOsample.txt Geoje Pohang Namhae YellowSea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mv \"PopulationMap.txt\" \"PopulationMap_batch12.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32madd_barcodes.py\u001b[0m*                   \u001b[01;32mmakeUniversalShell\u001b[0m*\r\n",
      "\u001b[01;32mcstacks_batch303\u001b[0m*                  \u001b[01;32mnew_cstacks_code\u001b[0m*\r\n",
      "\u001b[01;32mcstacks_generateCode_10-21.py\u001b[0m*     \u001b[01;32mnew_filenames1.txt\u001b[0m*\r\n",
      "\u001b[01;32mcstacks_generateCode2.py\u001b[0m*          \u001b[01;32mnew_filenames2.txt\u001b[0m*\r\n",
      "\u001b[01;32mcstacks_generateLineCountCode.py\u001b[0m*  \u001b[01;32mnew_sstacks_code\u001b[0m*\r\n",
      "\u001b[01;32mcstacks_replaceIDs.py\u001b[0m*             \u001b[01;32mnew_stacks_shell\u001b[0m*\r\n",
      "\u001b[01;32mcstacks_seqcountSamples\u001b[0m*           \u001b[01;32mPopulationMap_batch12.txt\u001b[0m*\r\n",
      "\u001b[01;32mcstacksShell_10-21\u001b[0m*                \u001b[01;32mPopulationMap_TEST.txt\u001b[0m*\r\n",
      "\u001b[01;32mfilenames_PEconversion.py\u001b[0m*         \u001b[01;32mprocess_radtags_nobarcode\u001b[0m*\r\n",
      "\u001b[01;32mfilenames_SRconversion.py\u001b[0m*         \u001b[01;32mradtags_ustacks_genShellPE.py\u001b[0m*\r\n",
      "\u001b[01;32mFileS1_MB.py\u001b[0m*                      \u001b[01;32mradtags_ustacks_genShell.py\u001b[0m*\r\n",
      "\u001b[01;32mgenPopMap.py\u001b[0m*                      \u001b[01;32mraw_data_example_nobarcode\u001b[0m*\r\n",
      "\u001b[01;32mL1_filename_conversion.txt\u001b[0m*        \u001b[01;32msamples_for_cstacks.txt\u001b[0m*\r\n",
      "\u001b[01;32mL1_mv_barcodesTOsampleTEST.txt\u001b[0m*    \u001b[01;32msstacks_generateCode.py\u001b[0m*\r\n",
      "\u001b[01;32mL1_mv_barcodesTOsample.txt\u001b[0m*        \u001b[01;32msstacks_generateShell10-26.py\u001b[0m*\r\n",
      "\u001b[34;42mLane1\u001b[0m/                             \u001b[01;32mustacks_generateShell.py\u001b[0m*\r\n",
      "\u001b[01;32mLane1barcodes.csv\u001b[0m*                 \u001b[01;32mustacks_replaceIDs.py\u001b[0m*\r\n",
      "\u001b[01;32mLane1_SortedFilesbySize.ods\u001b[0m*       \u001b[01;32mustacks_shell_lane1.txt\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can run the `populations` program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1\n"
     ]
    }
   ],
   "source": [
    "cd UCstacksL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fst kernel smoothing: off\r\n",
      "Bootstrap resampling: off\r\n",
      "Percent samples limit per population: 0.5\r\n",
      "Locus Population limit: 2\r\n",
      "Minimum stack depth: 5\r\n",
      "Log liklihood filtering: off; threshold: 0\r\n",
      "Minor allele frequency cutoff: 0\r\n",
      "Maximum observed heterozygosity cutoff: 1\r\n",
      "Applying Fst correction: none.\r\n",
      "Parsing population map...\r\n",
      "The population map contained 60 samples, 4 population(s), 1 group(s).\r\n",
      "Error: Unable to locate any file in input directory 'populations_temp/'.\r\n"
     ]
    }
   ],
   "source": [
    "!populations -b 12 \\\n",
    "-P populations_temp \\\n",
    "-M ../scripts/PopulationMap_batch12.txt \\\n",
    "-t 36 \\\n",
    "-r 0.50 \\\n",
    "-p 2 \\\n",
    "-m 5 \\\n",
    "--genepop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Missing the batch_12.catalog files. copied to populations_temp folder, tried again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fst kernel smoothing: off\r\n",
      "Bootstrap resampling: off\r\n",
      "Percent samples limit per population: 0.5\r\n",
      "Locus Population limit: 2\r\n",
      "Minimum stack depth: 5\r\n",
      "Log liklihood filtering: off; threshold: 0\r\n",
      "Minor allele frequency cutoff: 0\r\n",
      "Maximum observed heterozygosity cutoff: 1\r\n",
      "Applying Fst correction: none.\r\n",
      "Parsing population map...\r\n",
      "The population map contained 60 samples, 4 population(s), 1 group(s).\r\n",
      "Error: Unable to locate any file in input directory 'populations_temp/'.\r\n"
     ]
    }
   ],
   "source": [
    "!populations -b 12 \\\n",
    "-P populations_temp \\\n",
    "-M ../scripts/PopulationMap_batch12.txt \\\n",
    "-t 36 \\\n",
    "-r 0.50 \\\n",
    "-p 2 \\\n",
    "-m 5 \\\n",
    "--genepop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*maybe something to do with the names in the Population Map file?*\n",
    "Changed \"sampleID\" to \"sampleID.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populations:  ['Geoje', 'Pohang', 'Namhae', 'YellowSea']\r\n",
      "Population Map generated with  60 samples\r\n"
     ]
    }
   ],
   "source": [
    "!python genPopMap.py L1_mv_barcodesTOsample.txt Geoje Pohang Namhae YellowSea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mv \"PopulationMap.txt\" \"PopulationMap_batch12.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PO010715_06.1\tPohang\r\n",
      "PO010715_27.1\tPohang\r\n",
      "PO010715_28.1\tPohang\r\n",
      "PO010715_29.1\tPohang\r\n",
      "GE011215_08.1\tGeoje\r\n",
      "GE011215_09.1\tGeoje\r\n",
      "GE011215_14.1\tGeoje\r\n",
      "GE011215_15.1\tGeoje\r\n",
      "NA021015_16.1\tNamhae\r\n",
      "NA021015_21.1\tNamhae\r\n"
     ]
    }
   ],
   "source": [
    "!head PopulationMap_batch12.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1\n"
     ]
    }
   ],
   "source": [
    "cd ../UCstacksL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fst kernel smoothing: off\n",
      "Bootstrap resampling: off\n",
      "Percent samples limit per population: 0.5\n",
      "Locus Population limit: 2\n",
      "Minimum stack depth: 5\n",
      "Log liklihood filtering: off; threshold: 0\n",
      "Minor allele frequency cutoff: 0\n",
      "Maximum observed heterozygosity cutoff: 1\n",
      "Applying Fst correction: none.\n",
      "Parsing population map...\n",
      "The population map contained 60 samples, 4 population(s), 1 group(s).\n",
      "Reading the catalog...\n",
      "  Parsing populations_temp/batch_12.catalog.tags.tsv.gz\n",
      "  Parsing populations_temp/batch_12.catalog.snps.tsv.gz\n",
      "  Parsing populations_temp/batch_12.catalog.alleles.tsv.gz\n",
      "Reading matches to the catalog...\n",
      "  Parsing populations_temp/PO010715_06.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO010715_08.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO010715_10.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO010715_11.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO010715_17.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO010715_19.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO010715_27.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO010715_28.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO010715_29.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO020515_03.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO020515_05.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO020515_08.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO020515_09.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO020515_10.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO020515_14.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO020515_15.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO020515_16.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO020515_17.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO031715_13.1.matches.tsv.gz\n",
      "  Parsing populations_temp/PO031715_20.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE011215_01.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE011215_07.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE011215_08.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE011215_09.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE011215_10.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE011215_14.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE011215_15.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE011215_16.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE011215_20.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE011215_21.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE011215_24.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE011215_29.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE011215_30.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE012315_01.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE012315_03.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE012315_04.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE012315_05.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE012315_06.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE012315_08.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE012315_09.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE012315_10.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE012315_11.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE012315_17.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE012315_20.1.matches.tsv.gz\n",
      "  Parsing populations_temp/GE012315_22.1.matches.tsv.gz\n",
      "  Parsing populations_temp/NA021015_02.1.matches.tsv.gz\n",
      "  Parsing populations_temp/NA021015_03.1.matches.tsv.gz\n",
      "  Parsing populations_temp/NA021015_06.1.matches.tsv.gz\n",
      "  Parsing populations_temp/NA021015_08.1.matches.tsv.gz\n",
      "  Parsing populations_temp/NA021015_09.1.matches.tsv.gz\n",
      "  Parsing populations_temp/NA021015_10.1.matches.tsv.gz\n",
      "  Parsing populations_temp/NA021015_13.1.matches.tsv.gz\n",
      "  Parsing populations_temp/NA021015_14.1.matches.tsv.gz\n",
      "  Parsing populations_temp/NA021015_16.1.matches.tsv.gz\n",
      "  Parsing populations_temp/NA021015_17.1.matches.tsv.gz\n",
      "  Parsing populations_temp/NA021015_21.1.matches.tsv.gz\n",
      "  Parsing populations_temp/NA021015_22.1.matches.tsv.gz\n",
      "  Parsing populations_temp/YS121315_08.1.matches.tsv.gz\n",
      "  Parsing populations_temp/YS121315_10.1.matches.tsv.gz\n",
      "  Parsing populations_temp/YS121315_14.1.matches.tsv.gz\n",
      "Catalog is not reference aligned, arbitrarily ordering catalog loci.\n",
      "Working on 60 samples.\n",
      "Working on 4 population(s):\n",
      "    Pohang: PO010715_06.1, PO010715_08.1, PO010715_10.1, PO010715_11.1, PO010715_17.1, PO010715_19.1, PO010715_27.1, PO010715_28.1, PO010715_29.1, PO020515_03.1, PO020515_05.1, PO020515_08.1, PO020515_09.1, PO020515_10.1, PO020515_14.1, PO020515_15.1, PO020515_16.1, PO020515_17.1, PO031715_13.1, PO031715_20.1\n",
      "    Geoje: GE011215_01.1, GE011215_07.1, GE011215_08.1, GE011215_09.1, GE011215_10.1, GE011215_14.1, GE011215_15.1, GE011215_16.1, GE011215_20.1, GE011215_21.1, GE011215_24.1, GE011215_29.1, GE011215_30.1, GE012315_01.1, GE012315_03.1, GE012315_04.1, GE012315_05.1, GE012315_06.1, GE012315_08.1, GE012315_09.1, GE012315_10.1, GE012315_11.1, GE012315_17.1, GE012315_20.1, GE012315_22.1\n",
      "    Namhae: NA021015_02.1, NA021015_03.1, NA021015_06.1, NA021015_08.1, NA021015_09.1, NA021015_10.1, NA021015_13.1, NA021015_14.1, NA021015_16.1, NA021015_17.1, NA021015_21.1, NA021015_22.1\n",
      "    YellowSea: YS121315_08.1, YS121315_10.1, YS121315_14.1\n",
      "Working on 1 group(s) of populations:\n",
      "    defaultgrp: Pohang, Geoje, Namhae, YellowSea\n",
      "Populating observed haplotypes for 60 samples, 105442 loci.\n",
      "Removed 33 samples from loci that are below the minimum stack depth of 5x\n",
      "Removing 67138 loci that did not pass sample/population constraints... retained 38304 loci.\n",
      "Loading model outputs for 60 samples, 38304 loci.\n",
      "  Parsing populations_temp/PO010715_06.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO010715_08.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO010715_10.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO010715_11.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO010715_17.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO010715_19.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO010715_27.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO010715_28.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO010715_29.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO020515_03.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO020515_05.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO020515_08.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO020515_09.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO020515_10.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO020515_14.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO020515_15.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO020515_16.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO020515_17.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO031715_13.1.models.tsv.gz\n",
      "  Parsing populations_temp/PO031715_20.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE011215_01.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE011215_07.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE011215_08.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE011215_09.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE011215_10.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE011215_14.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE011215_15.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE011215_16.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE011215_20.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE011215_21.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE011215_24.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE011215_29.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE011215_30.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE012315_01.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE012315_03.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE012315_04.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE012315_05.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE012315_06.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE012315_08.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE012315_09.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE012315_10.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE012315_11.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE012315_17.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE012315_20.1.models.tsv.gz\n",
      "  Parsing populations_temp/GE012315_22.1.models.tsv.gz\n",
      "  Parsing populations_temp/NA021015_02.1.models.tsv.gz\n",
      "  Parsing populations_temp/NA021015_03.1.models.tsv.gz\n",
      "  Parsing populations_temp/NA021015_06.1.models.tsv.gz\n",
      "  Parsing populations_temp/NA021015_08.1.models.tsv.gz\n",
      "  Parsing populations_temp/NA021015_09.1.models.tsv.gz\n",
      "  Parsing populations_temp/NA021015_10.1.models.tsv.gz\n",
      "  Parsing populations_temp/NA021015_13.1.models.tsv.gz\n",
      "  Parsing populations_temp/NA021015_14.1.models.tsv.gz\n",
      "  Parsing populations_temp/NA021015_16.1.models.tsv.gz\n",
      "  Parsing populations_temp/NA021015_17.1.models.tsv.gz\n",
      "  Parsing populations_temp/NA021015_21.1.models.tsv.gz\n",
      "  Parsing populations_temp/NA021015_22.1.models.tsv.gz\n",
      "  Parsing populations_temp/YS121315_08.1.models.tsv.gz\n",
      "  Parsing populations_temp/YS121315_10.1.models.tsv.gz\n",
      "  Parsing populations_temp/YS121315_14.1.models.tsv.gz\n",
      "Generating nucleotide-level summary statistics for population 'Pohang'\n",
      "Population 'Pohang' contained 104 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population 'Geoje'\n",
      "Population 'Geoje' contained 110 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population 'Namhae'\n",
      "Population 'Namhae' contained 90 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population 'YellowSea'\n",
      "Population 'YellowSea' contained 38 incompatible loci -- more than two alleles present.\n",
      "Tallying loci across populations...done.\n",
      "Pruned 662 variant sites due to filter constraints (more with --verbose).\n",
      "Removing 306 additional loci for which all variant sites were filtered... retained 37998 loci.\n",
      "Regenerating nucleotide-level summary statistics for population 'Pohang'\n",
      "Population 'Pohang' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population 'Geoje'\n",
      "Population 'Geoje' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population 'Namhae'\n",
      "Population 'Namhae' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population 'YellowSea'\n",
      "Population 'YellowSea' contained 0 incompatible loci -- more than two alleles present.\n",
      "Re-tallying loci across populations...done.\n",
      "Generating haplotype-level summary statistics for population 'Pohang'\n",
      "Generating haplotype-level summary statistics for population 'Geoje'\n",
      "Generating haplotype-level summary statistics for population 'Namhae'\n",
      "Generating haplotype-level summary statistics for population 'YellowSea'\n",
      "Writing 37998 loci to summary statistics file, 'populations_temp/batch_12.sumstats.tsv'\n",
      "Writing 37998 loci to observed haplotype file, 'populations_temp/batch_12.haplotypes.tsv'\n",
      "Writing population data to GenePop file 'populations_temp/batch_12.genepop'\n",
      "Populations is done.\n"
     ]
    }
   ],
   "source": [
    "!populations -b 12 \\\n",
    "-P populations_temp \\\n",
    "-M ../scripts/PopulationMap_batch12.txt \\\n",
    "-t 36 \\\n",
    "-r 0.50 \\\n",
    "-p 2 \\\n",
    "-m 5 \\\n",
    "--genepop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir populations_batch12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mv populations_temp/batch_12.genepop populations_batch12/batch_12.genepop\n",
    "!mv populations_temp/batch_12.hapstats.tsv populations_batch12/batch_12.hapstats.tsv\n",
    "!mv populations_temp/batch_12.sumstats.tsv populations_batch12/batch_12.sumstats.tsv\n",
    "!mv populations_temp/batch_12.haplotypes.tsv populations_batch12/batch_12.haplotypes.tsv\n",
    "!mv populations_temp/batch_12.populations.log populations_batch12/batch_12.populations.log\n",
    "!mv populations_temp/batch_12.sumstats_summary.tsv populations_batch12/batch_12.sumstats_summary.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have batches 303, 330, 1030, 230 run over night, I made a shell script (manually, no python code to generate). \n",
    "\n",
    "Deleted batch_12.catalog, .matches files, and Yellow Sea files from populations_temp folder. Copied in batch_303, .matches files from batch_303. \n",
    "<br>\n",
    "<br>\n",
    "Copied sstacks output for each of the batches 330, 1030, 230 into the appropriate stacks folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populations:  ['Geoje', 'Namhae', 'Pohang']\r\n",
      "Population Map generated with  57 samples\r\n"
     ]
    }
   ],
   "source": [
    "!python genPopMap.py L1_mv_barcodesTOsampleTEST.txt Geoje Namhae Pohang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mv \"PopulationMap.txt\" \"PopulationMapTEST.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PO010715_06.1\tPohang\r\n",
      "PO010715_27.1\tPohang\r\n",
      "PO010715_28.1\tPohang\r\n",
      "PO010715_29.1\tPohang\r\n",
      "GE011215_08.1\tGeoje\r\n",
      "GE011215_09.1\tGeoje\r\n",
      "GE011215_14.1\tGeoje\r\n",
      "GE011215_15.1\tGeoje\r\n",
      "NA021015_16.1\tNamhae\r\n",
      "NA021015_21.1\tNamhae\r\n"
     ]
    }
   ],
   "source": [
    "!head PopulationMapTEST.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created shell script and moved to UCstacksL1 directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1\n"
     ]
    }
   ],
   "source": [
    "cd ../UCstacksL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "\r\n",
      "populations -b 303 \\\r\n",
      "-P populations_temp \\\r\n",
      "-M ../scripts/PopulationMapTEST.txt \\\r\n",
      "-t 36 \\\r\n",
      "-r 0.50 \\\r\n",
      "-p 2 \\\r\n",
      "-m 5 \\\r\n",
      "--genepop \\\r\n"
     ]
    }
   ],
   "source": [
    "!head populationsShell_11-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x populationsShell_11-1 #make shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "./ populationsShell_11-1 #ran on command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11/2/2016\n",
    "\n",
    "### (2) Populations output\n",
    "\n",
    "[Evernote page](http://www.evernote.com/l/AoqT8ohibn9Dvafdz-TxhHAPav412kkvMZc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/mf-fish546-2016/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1\n"
     ]
    }
   ],
   "source": [
    "cd UCstacksL1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mcstacks_out4.stderr\u001b[0m*        \u001b[34;42mpopulations_temp\u001b[0m/        \u001b[34;42msstacks_batch330\u001b[0m/\r\n",
      "\u001b[01;32mcstacks_Shell_10-21\u001b[0m*        \u001b[34;42msstacks_batch1030\u001b[0m/       \u001b[01;32msstacks_shell_10-26\u001b[0m*\r\n",
      "\u001b[34;42mpopulations_batch12\u001b[0m/        \u001b[34;42msstacks_batch12\u001b[0m/         \u001b[34;42mstacks_base\u001b[0m/\r\n",
      "\u001b[01;32mpopulations_out_batch1030\u001b[0m*  \u001b[01;32msstacks_batch12_shell\u001b[0m*   \u001b[34;42mstacks_m10\u001b[0m/\r\n",
      "\u001b[01;32mpopulations_out_batch230\u001b[0m*   \u001b[34;42msstacks_batch230\u001b[0m/        \u001b[34;42mstacks_M2\u001b[0m/\r\n",
      "\u001b[01;32mpopulations_out_batch303\u001b[0m*   \u001b[34;42msstacks_batch300\u001b[0m/        \u001b[34;42mstacks_m3\u001b[0m/\r\n",
      "\u001b[01;32mpopulations_out_batch330\u001b[0m*   \u001b[01;32msstacks_batch300_shell\u001b[0m*\r\n",
      "\u001b[01;32mpopulationsShell_11-1\u001b[0m*      \u001b[34;42msstacks_batch303\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of making a separate directory for each batch's `populations` files, I'm going to put them all in a single directory \"populations/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mcstacks_out4.stderr\u001b[0m*    \u001b[34;42msstacks_batch12\u001b[0m/         \u001b[01;32msstacks_shell_10-26\u001b[0m*\r\n",
      "\u001b[01;32mcstacks_Shell_10-21\u001b[0m*    \u001b[01;32msstacks_batch12_shell\u001b[0m*   \u001b[34;42mstacks_base\u001b[0m/\r\n",
      "\u001b[34;42mpopulations\u001b[0m/            \u001b[34;42msstacks_batch230\u001b[0m/        \u001b[34;42mstacks_m10\u001b[0m/\r\n",
      "\u001b[34;42mpopulations_batch12\u001b[0m/    \u001b[34;42msstacks_batch300\u001b[0m/        \u001b[34;42mstacks_M2\u001b[0m/\r\n",
      "\u001b[01;32mpopulationsShell_11-1\u001b[0m*  \u001b[01;32msstacks_batch300_shell\u001b[0m*  \u001b[34;42mstacks_m3\u001b[0m/\r\n",
      "\u001b[34;42mpopulations_temp\u001b[0m/       \u001b[34;42msstacks_batch303\u001b[0m/\r\n",
      "\u001b[34;42msstacks_batch1030\u001b[0m/      \u001b[34;42msstacks_batch330\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then moved the stderr files and all of the `populations` output files into the appropriate directory. I also deleted the extra copies of the sstacks files that were moved to the stacks/ folders for the analysis above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mcstacks_out4.stderr\u001b[0m*    \u001b[34;42msstacks_batch12\u001b[0m/         \u001b[34;42msstacks_batch330\u001b[0m/\r\n",
      "\u001b[01;32mcstacks_Shell_10-21\u001b[0m*    \u001b[01;32msstacks_batch12_shell\u001b[0m*   \u001b[01;32msstacks_shell_10-26\u001b[0m*\r\n",
      "\u001b[34;42mpopulations\u001b[0m/            \u001b[34;42msstacks_batch230\u001b[0m/        \u001b[34;42mstacks_base\u001b[0m/\r\n",
      "\u001b[01;32mpopulationsShell_11-1\u001b[0m*  \u001b[34;42msstacks_batch300\u001b[0m/        \u001b[34;42mstacks_m10\u001b[0m/\r\n",
      "\u001b[34;42mpopulations_temp\u001b[0m/       \u001b[01;32msstacks_batch300_shell\u001b[0m*  \u001b[34;42mstacks_M2\u001b[0m/\r\n",
      "\u001b[34;42msstacks_batch1030\u001b[0m/      \u001b[34;42msstacks_batch303\u001b[0m/        \u001b[34;42mstacks_m3\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/populations\n"
     ]
    }
   ],
   "source": [
    "cd populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mbatch_1030.genepop\u001b[0m*               \u001b[01;32mbatch_230.sumstats.tsv\u001b[0m*\r\n",
      "\u001b[01;32mbatch_1030.haplotypes.tsv\u001b[0m*        \u001b[01;32mbatch_303.genepop\u001b[0m*\r\n",
      "\u001b[01;32mbatch_1030.hapstats.tsv\u001b[0m*          \u001b[01;32mbatch_303.haplotypes.tsv\u001b[0m*\r\n",
      "\u001b[01;32mbatch_1030.populations.log\u001b[0m*       \u001b[01;32mbatch_303.hapstats.tsv\u001b[0m*\r\n",
      "\u001b[01;32mbatch_1030.sumstats_summary.tsv\u001b[0m*  \u001b[01;32mbatch_303.populations.log\u001b[0m*\r\n",
      "\u001b[01;32mbatch_1030.sumstats.tsv\u001b[0m*          \u001b[01;32mbatch_303.sumstats_summary.tsv\u001b[0m*\r\n",
      "\u001b[01;32mbatch_12.genepop\u001b[0m*                 \u001b[01;32mbatch_303.sumstats.tsv\u001b[0m*\r\n",
      "\u001b[01;32mbatch_12.haplotypes.tsv\u001b[0m*          \u001b[01;32mbatch_330.genepop\u001b[0m*\r\n",
      "\u001b[01;32mbatch_12.hapstats.tsv\u001b[0m*            \u001b[01;32mbatch_330.haplotypes.tsv\u001b[0m*\r\n",
      "\u001b[01;32mbatch_12.populations.log\u001b[0m*         \u001b[01;32mbatch_330.hapstats.tsv\u001b[0m*\r\n",
      "\u001b[01;32mbatch_12.sumstats_summary.tsv\u001b[0m*    \u001b[01;32mbatch_330.populations.log\u001b[0m*\r\n",
      "\u001b[01;32mbatch_12.sumstats.tsv\u001b[0m*            \u001b[01;32mbatch_330.sumstats_summary.tsv\u001b[0m*\r\n",
      "\u001b[01;32mbatch_230.genepop\u001b[0m*                \u001b[01;32mbatch_330.sumstats.tsv\u001b[0m*\r\n",
      "\u001b[01;32mbatch_230.haplotypes.tsv\u001b[0m*         \u001b[01;32mpopulations_out_batch1030\u001b[0m*\r\n",
      "\u001b[01;32mbatch_230.hapstats.tsv\u001b[0m*           \u001b[01;32mpopulations_out_batch230\u001b[0m*\r\n",
      "\u001b[01;32mbatch_230.populations.log\u001b[0m*        \u001b[01;32mpopulations_out_batch303\u001b[0m*\r\n",
      "\u001b[01;32mbatch_230.sumstats_summary.tsv\u001b[0m*   \u001b[01;32mpopulations_out_batch330\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls #list of the `populations` output files for batch 1030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now have only one more batch to run through `populations` : batch 300, from stacks_base. \n",
    "\n",
    "I copied the ustacks, sstacks, and cstacks files for batch_300 from the stacks_base and the sstacks_batch300 folders into the populations_temp folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!populations -b 300 \\\n",
    "-P populations_temp \\\n",
    "-M ../scripts/PopulationMapTest.txt \\\n",
    "-t 36 \\\n",
    "-r 0.50 \\\n",
    "-p 2 \\\n",
    "-m 5 \\\n",
    "--genepop \\\n",
    "2>> populations_output_batch300.stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I copied the stderr file and the 6 output files into the `populations` folder. I then deleted the \"populations_temp/\" folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/populations\n"
     ]
    }
   ],
   "source": [
    "cd populations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the 6 types of output files from `populations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "populations -b 303 -P populations_temp -M ../scripts/PopulationMapTEST.txt -t 36 -r 0.50 -p 2 -m 5 --genepop\r\n",
      "populations version 1.42 executed 2016-11-01 18:14:28\r\n",
      "\r\n",
      "# Distribution of population loci.\r\n",
      "# Distribution of valid loci matched to catalog locus.\r\n",
      "# Valid samples at locus\tCount\r\n",
      "0\t4\r\n",
      "1\t66206\r\n",
      "2\t9379\r\n",
      "3\t3480\r\n",
      "4\t1985\r\n",
      "5\t1500\r\n",
      "6\t1201\r\n",
      "7\t1060\r\n",
      "8\t896\r\n"
     ]
    }
   ],
   "source": [
    "# File 1: populations.log\n",
    "!head -n 15 batch_303.populations.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Pohang\tPO010715_06.1,PO010715_08.1,PO010715_10.1,PO010715_11.1,PO010715_17.1,PO010715_19.1,PO010715_27.1,PO010715_28.1,PO010715_29.1,PO020515_03.1,PO020515_05.1,PO020515_08.1,PO020515_09.1,PO020515_10.1,PO020515_14.1,PO020515_15.1,PO020515_16.1,PO020515_17.1,PO031715_13.1,PO031715_20.1\r\n",
      "# Geoje\tGE011215_01.1,GE011215_07.1,GE011215_08.1,GE011215_09.1,GE011215_10.1,GE011215_14.1,GE011215_15.1,GE011215_16.1,GE011215_20.1,GE011215_21.1,GE011215_24.1,GE011215_29.1,GE011215_30.1,GE012315_01.1,GE012315_03.1,GE012315_04.1,GE012315_05.1,GE012315_06.1,GE012315_08.1,GE012315_09.1,GE012315_10.1,GE012315_11.1,GE012315_17.1,GE012315_20.1,GE012315_22.1\r\n",
      "# Namhae\tNA021015_02.1,NA021015_03.1,NA021015_06.1,NA021015_08.1,NA021015_09.1,NA021015_10.1,NA021015_13.1,NA021015_14.1,NA021015_16.1,NA021015_17.1,NA021015_21.1,NA021015_22.1\r\n",
      "# Batch ID \tLocus ID\tChr\tBP\tCol\tPop ID\tP Nuc\tQ Nuc\tN\tP\tObs Het\tObs Hom\tExp Het\tExp Hom\tPi\tSmoothed Pi\tSmoothed Pi P-value\tFis\tSmoothed Fis\tSmoothed Fis P-value\tPrivate\r\n",
      "303\t2\tun\t275\t131\tPohang\tT\tG\t16\t0.96875000\t0.0625\t0.9375\t0.0605\t0.9395\t0.0625\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t1\r\n",
      "303\t2\tun\t275\t131\tGeoje\tT\t-\t18\t1.00000000\t0.0000\t1.0000\t0.0000\t1.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0\r\n"
     ]
    }
   ],
   "source": [
    "# File 2: sumstats.tsv\n",
    "!head -n 6 batch_303.sumstats.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Variant positions\r\n",
      "# Pop ID\tPrivate\tNum Indv\tVar\tStdErr\tP\tVar\tStdErr\tObs Het\tVar\tStdErr\tObs Hom\tVar\tStdErr\tExp Het\tVar\tStdErr\tExp Hom\tVar\tStdErr\tPi\tVar\tStdErr\tFis\tVar\tStdErr\r\n",
      "Pohang\t3324\t17.8251\t8.7060\t0.0160\t0.8663\t0.0202\t0.0008\t0.1714\t0.0365\t0.0010\t0.8286\t0.0365\t0.0010\t0.1912\t0.0284\t0.0009\t0.8088\t0.0284\t0.0009\t0.1970\t0.0302\t0.0009\t0.0927\t9.5690\t0.0160\r\n",
      "Geoje\t2790\t22.3540\t12.2096\t0.0190\t0.8670\t0.0204\t0.0008\t0.1739\t0.0373\t0.0011\t0.8261\t0.0373\t0.0011\t0.1898\t0.0288\t0.0009\t0.8102\t0.0288\t0.0009\t0.1943\t0.0302\t0.0009\t0.0780\t9.7361\t0.0190\r\n",
      "Namhae\t2437\t10.6882\t3.3275\t0.0099\t0.8658\t0.0211\t0.0008\t0.1759\t0.0415\t0.0011\t0.8241\t0.0415\t0.0011\t0.1902\t0.0298\t0.0009\t0.8098\t0.0298\t0.0009\t0.2000\t0.0330\t0.0010\t0.0750\t13.6856\t0.0099\r\n",
      "# All positions (variant and fixed)\r\n",
      "# Pop ID\tPrivate\tSites\tVariant Sites\tPolymorphic Sites\t% Polymorphic Loci\tNum Indv\tVar\tStdErr\tP\tVar\tStdErr\tObs Het\tVar\tStdErr\tObs Hom\tVar\tStdErr\tExp Het\tVar\tStdErr\tExp Hom\tVar\tStdErr\tPi\tVar\tStdErr\tFis\tVar\tStdErr\r\n",
      "Pohang\t3324\t5244448.0000\t34119.0000\t27699.0000\t0.5282\t18.7299\t6.1391\t0.0011\t0.9991\t0.0002\t0.0000\t0.0011\t0.0004\t0.0000\t0.9989\t0.0004\t0.0000\t0.0012\t0.0004\t0.0000\t0.9988\t0.0004\t0.0000\t0.0013\t0.0004\t0.0000\t0.0006\t48.7503\t0.0011\r\n",
      "Geoje\t2790\t5232279.0000\t33775.0000\t27274.0000\t0.5213\t23.4551\t8.6890\t0.0013\t0.9991\t0.0002\t0.0000\t0.0011\t0.0004\t0.0000\t0.9989\t0.0004\t0.0000\t0.0012\t0.0004\t0.0000\t0.9988\t0.0004\t0.0000\t0.0013\t0.0004\t0.0000\t0.0005\t48.7523\t0.0013\r\n",
      "Namhae\t2437\t5261379.0000\t34099.0000\t24847.0000\t0.4723\t11.2438\t2.2978\t0.0007\t0.9991\t0.0003\t0.0000\t0.0011\t0.0005\t0.0000\t0.9989\t0.0005\t0.0000\t0.0012\t0.0004\t0.0000\t0.9988\t0.0004\t0.0000\t0.0013\t0.0005\t0.0000\t0.0005\t48.7761\t0.0007\r\n"
     ]
    }
   ],
   "source": [
    "#File 3: sumstats_summary.tsv\n",
    "!head -n 15 batch_303.sumstats_summary.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Pohang\tPO010715_06.1,PO010715_08.1,PO010715_10.1,PO010715_11.1,PO010715_17.1,PO010715_19.1,PO010715_27.1,PO010715_28.1,PO010715_29.1,PO020515_03.1,PO020515_05.1,PO020515_08.1,PO020515_09.1,PO020515_10.1,PO020515_14.1,PO020515_15.1,PO020515_16.1,PO020515_17.1,PO031715_13.1,PO031715_20.1\r\n",
      "# Geoje\tGE011215_01.1,GE011215_07.1,GE011215_08.1,GE011215_09.1,GE011215_10.1,GE011215_14.1,GE011215_15.1,GE011215_16.1,GE011215_20.1,GE011215_21.1,GE011215_24.1,GE011215_29.1,GE011215_30.1,GE012315_01.1,GE012315_03.1,GE012315_04.1,GE012315_05.1,GE012315_06.1,GE012315_08.1,GE012315_09.1,GE012315_10.1,GE012315_11.1,GE012315_17.1,GE012315_20.1,GE012315_22.1\r\n",
      "# Namhae\tNA021015_02.1,NA021015_03.1,NA021015_06.1,NA021015_08.1,NA021015_09.1,NA021015_10.1,NA021015_13.1,NA021015_14.1,NA021015_16.1,NA021015_17.1,NA021015_21.1,NA021015_22.1\r\n",
      "# Batch ID \tLocus ID\tChr\tBP\tPop ID\tN\tHaplotype Cnt\tGene Diversity\tSmoothed Gene Diversity\tSmoothed Gene Diversity P-value\tHaplotype Diversity\tSmoothed Haplotype Diversity\tSmoothed Haplotype Diversity P-value\tHaplotypes\r\n",
      "303\t2\tun\t144\tPohang\t36\t2\t0.0556\t0.0000\t0.0000\t0.0556\t0.0000\t0.0000\tG:1;T:35\r\n",
      "303\t6\tun\t712\tPohang\t40\t1\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\tGTC:40\r\n"
     ]
    }
   ],
   "source": [
    "#File 4: hapstats.tsv\n",
    "!head -n 6 batch_303.hapstats.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog ID\tCnt\tPO010715_06.1\tPO010715_08.1\tPO010715_10.1\tPO010715_11.1\tPO010715_17.1\tPO010715_19.1\tPO010715_27.1\tPO010715_28.1\tPO010715_29.1\tPO020515_03.1\tPO020515_05.1\tPO020515_08.1\tPO020515_09.1\tPO020515_10.1\tPO020515_14.1\tPO020515_15.1\tPO020515_16.1\tPO020515_17.1\tPO031715_13.1\tPO031715_20.1\tGE011215_01.1\tGE011215_07.1\tGE011215_08.1\tGE011215_09.1\tGE011215_10.1\tGE011215_14.1\tGE011215_15.1\tGE011215_16.1\tGE011215_20.1\tGE011215_21.1\tGE011215_24.1\tGE011215_29.1\tGE011215_30.1\tGE012315_01.1\tGE012315_03.1\tGE012315_04.1\tGE012315_05.1\tGE012315_06.1\tGE012315_08.1\tGE012315_09.1\tGE012315_10.1\tGE012315_11.1\tGE012315_17.1\tGE012315_20.1\tGE012315_22.1\tNA021015_02.1\tNA021015_03.1\tNA021015_06.1\tNA021015_08.1\tNA021015_09.1\tNA021015_10.1\tNA021015_13.1\tNA021015_14.1\tNA021015_16.1\tNA021015_17.1\tNA021015_21.1\tNA021015_22.1\r\n",
      "1\t48\t-\tconsensus\tconsensus\tconsensus\tconsensus\t-\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\t-\tconsensus\tconsensus\t-\tconsensus\t-\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\t-\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\t-\tconsensus\tconsensus\tconsensus\t-\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\tconsensus\t-\tconsensus\r\n",
      "2\t51\tT\tT\tT\tT\tT\tT\tT\tT\tT\tT\tT\tT\t-\tT\tT\tG/T\tT\tT\tT\t-\tT\tT\tT\tT\tT\tT\tT\tT\t-\tT\tT\tT\tT\tT\tT\tT\tT\tT\tT\tT\tT\tT\tT\t-\t-\tT\tT\tT\t-\tT\tT\tT\tT\tT\tT\tT\tT\r\n"
     ]
    }
   ],
   "source": [
    "#File 5: haplotypes.tsv\n",
    "!head -n 3 batch_303.haplotypes.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#File 6: genepop\n",
    "#This is a .csv file with an extremely large heading, so will open in excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Changing -m in ustacks\n",
    "\n",
    "I'll first focus on the changed **-m** parameter. \n",
    "\n",
    "These are batches 303 (m = 5), 330 (m = 3), and 1030 (m = 10). \n",
    "\n",
    "**(1)** Count the number of loci retained in each batch\n",
    "\n",
    "    - I went into each genepop file and copied the loci in the first row to a new text file, saved as `batch_xloci.txt`. I then created the following python script, which uses the fact that each locus is separated by a \",\" to count the number of loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/mf-fish546-2016/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/populations\n"
     ]
    }
   ],
   "source": [
    "cd ../../UCstacksL1/populations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\r",
      "\r\n",
      "batchFile = open(sys.argv[1], \"r\")\r",
      "\r\n",
      "\r",
      "\r\n",
      "#read row of loci names into a string\r",
      "\r\n",
      "Filestring = batchFile.readline()\r",
      "\r\n",
      "\r",
      "\r\n",
      "#count the number of \",\" in the file and adds 1 (the last locus doesn't have a comma after it)\r",
      "\r\n",
      "numLoci = Filestring.count(\",\") + 1 \r",
      "\r\n",
      "print \"The number of loci in \", sys.argv[1], \"is: \", numLoci\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head LocusCount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of loci in  batch_303loci.txt is:  34892\n",
      "The number of loci in  batch_330loci.txt is:  34667\n",
      "The number of loci in  batch_1030loci.txt is:  32440\n"
     ]
    }
   ],
   "source": [
    "!python LocusCount.py batch_303loci.txt\n",
    "!python LocusCount.py batch_330loci.txt\n",
    "!python LocusCount.py batch_1030loci.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry is NOT a single locus; it is a locus_SNP@locus. Therefore if there are multiple SNPs at the same locus, the code above actually counts a single locus more than once. I adjusted the script to count only unique loci: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\r",
      "\r\n",
      "batchFile = open(sys.argv[1], \"r\")\r",
      "\r\n",
      "\r",
      "\r\n",
      "#read row of loci names into a string\r",
      "\r\n",
      "Filestring = batchFile.readline()\r",
      "\r\n",
      "\r",
      "\r\n",
      "lociList = Filestring.split(\",\")\r",
      "\r\n",
      "UniqueLoci = []\r",
      "\r\n",
      "for locus in lociList:\r",
      "\r\n",
      "\ttemplocus = locus.split(\"_\")\r",
      "\r\n",
      "\tif templocus[0] not in UniqueLoci:\r",
      "\r\n",
      "\t\tUniqueLoci.append(templocus[0])\r",
      "\r\n",
      "\r",
      "\r\n",
      "#count the number of \",\" in the file and adds 1 (the last locus doesn't have a comma after it)\r",
      "\r\n",
      "numLoci = Filestring.count(\",\") + 1 \r",
      "\r\n",
      "print \"The number of loci in \", sys.argv[1], \"is: \", len(UniqueLoci)\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#adjust to count UNIQUE loci\n",
    "!head -n 20 LocusCount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of loci in  batch_303loci.txt is:  18720\n",
      "The number of loci in  batch_330loci.txt is:  18865\n",
      "The number of loci in  batch_1030loci.txt is:  17565\n"
     ]
    }
   ],
   "source": [
    "!python LocusCount.py batch_303loci.txt\n",
    "!python LocusCount.py batch_330loci.txt\n",
    "!python LocusCount.py batch_1030loci.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I'm looking at the **.sumstats_summary** files. This has the observed and expected heterozygosities for each population, across all batches. I created a chart in excel to compare, and have all graphs on the [Evernote notebook](https://www.evernote.com/Home.action#n=93f28862-6e7f-43bd-a7dd-cfe4f184700f&ses=4&sh=2&sds=5&)\n",
    "\n",
    "I'm a little concerned by the behavior of the first population (Pohang), since the trends between batches don't seem to hold for that population. \n",
    "\n",
    "Trend: Higher **-m** value = greater Ho and He. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I used the **.sumstats** file. I'm mostly interested in observed v. expected heterozygosity at each locus. I'll start by averaging heterozygosity across populations, instead of doing Ho and He per locus per population. I can do this by copying and pasting .sumstats into excel for each batch.\n",
    "\n",
    "\n",
    "I then deleted all columns except for the (batch #, locus.ID, Chr, BP, Col, Pop.ID, Obs.Het, Exp.Het). And copied these into a text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/mf-fish546-2016/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/populations\n"
     ]
    }
   ],
   "source": [
    "cd ../../UCstacksL1/populations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID \tLocus ID\tChr\tBP\tCol\tPop ID\tObs Het\tExp Het\r",
      "\r\n",
      "1030\t2\tun\t170\t26\tPohang\t0.0667\t0.0644\r",
      "\r\n",
      "1030\t2\tun\t170\t26\tGeoje\t0.2\t0.18\r",
      "\r\n",
      "1030\t2\tun\t170\t26\tNamhae\t0.2857\t0.2449\r",
      "\r\n",
      "1030\t2\tun\t190\t46\tPohang\t0.0667\t0.0644\r",
      "\r\n",
      "1030\t2\tun\t190\t46\tGeoje\t0.0476\t0.0465\r",
      "\r\n",
      "1030\t2\tun\t190\t46\tNamhae\t0.1429\t0.1327\r",
      "\r\n",
      "1030\t2\tun\t199\t55\tPohang\t0.7857\t0.4974\r",
      "\r\n",
      "1030\t2\tun\t199\t55\tGeoje\t0.6316\t0.4321\r",
      "\r\n",
      "1030\t2\tun\t199\t55\tNamhae\t0.6667\t0.5\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head batch_1030Het.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I imported this into R, and then found the mean observed and expected heterozygosity for each locus. I subtracted these to find the difference between observed and expected heterozygosity (if diff is +, observed is > expected). The R script is in the populations folder. \n",
    "\n",
    "\n",
    "I made some charts to compare within and between batches... which can be found in the middle (11/2/2016) of this [Evernote notebook](http://www.evernote.com/l/AoqT8ohibn9Dvafdz-TxhHAPav412kkvMZc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) BOWTIE\n",
    "\n",
    "\n",
    "I'm running bowtie to align the sequences produced by stacks against themselves. This will identify repeats and further quality screen the sequences that I would want to use as a reference genome. \n",
    "<br>\n",
    "<br>\n",
    "**STEP ONE:** In order to do this, I first must extract the sequences of all of the unique loci (found in the genepop file above) from the appropriate `batch.catalog.tags.tsv.gz` file. \n",
    "<br>\n",
    "I then have to make a fasta file with the locus number and the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/populations'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/stacks_base\n"
     ]
    }
   ],
   "source": [
    "cd ../stacks_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cstacks version 1.42; catalog generated on 2016-10-21 17:48:50\r\n",
      "0\t303\t1\t\t0\t+\tconsensus\t0\t16_1,27_11076,55_34661,6_14152,17_15533,47_11766,39_24551,8_29358,7_1007,30_6350,34_24885,19_26958,42_33545,35_42596,40_4549,41_13117,22_14682,9_25225,32_5966,46_32018,51_39840,36_1567,13_4048,43_19132,45_38285,44_26346,15_7664,33_32513\tTGCAGGCCCTCTGTGCTGTAGCCTCTCATGTCTGGCCAGGGCCCGTGTGGTGATGCATTAGAGGGCCTGTGATTGTACTGGAGGACTCCTCCTGACAAGCCGTTTGACACCTCTCAACAGGGTCAGAGGTGTCTGTCTGCCA\t0\t0\t0\t0\r\n",
      "0\t303\t2\t\t0\t+\tconsensus\t0\t16_2,27_24706,55_35316,23_12768,6_15274,17_41621,47_3453,39_20885,8_32240,7_22096,30_15755,34_23525,10_2842,19_27358,42_36621,35_28687,40_20006,41_1490,22_20847,9_39264,32_33975,46_6637,51_36398,36_8199,13_1521,43_36300,45_22727,44_8205,15_37715,33_32359\tTGCAGGTAGATTGGGTCGGGGGGGGGGGGTCCCAACCAAGAACCCTTGGTCATGTTGGGAGTCAAACACAATAATGACTAATAATCCGTCTGCATTATCCCATGATCTACTAACGGTGTGTACGCGGCGCGTGTGAATTTGT\t0\t0\t0\t0\r\n",
      "0\t303\t3\t\t0\t+\tconsensus\t0\t16_3,27_26582,55_13854,23_4538,6_5853,17_12044,47_27320,39_37715,8_15964,7_34014,30_13203,34_40665,10_19179,19_4090,42_31974,35_10863,40_5032,41_39834,22_18006,9_26311,32_27306,51_8558,36_14109,13_26487,44_15640,33_34142\tTGCAGGAGAGACACACACACACACACACACACACACATACACACACGCAATACACACGCACAAACATACGCACAATATACACACACACACACACGATACACACAATACACACACACAATACACAATACACACAATACATACA\t0\t0\t0\t0\r\n",
      "0\t303\t4\t\t0\t+\tconsensus\t0\t16_4,27_15853,55_9617,23_9443,6_39980,17_11053,47_20088,39_4671,8_39250,7_29268,30_39075,34_9457,10_21300,19_38642,42_2360,35_42412,40_32699,41_25841,22_41094,9_33846,32_35017,46_37490,51_2432,36_5943,13_28362,43_14591,45_7141,44_1560,15_28786,33_13696\tTGCAGGACAGATGTGCACGTCGGTCAGCGTCAGCCGTCTATTCGTATCTCAGCTGGAGATTTGAAGTGAACCCCTTGAGTCGCCTGCCATCCATCATCGCTCTGTTTACAATAAAATAACTGCAAAAGGTTCGGTTAAGGAC\t0\t0\t0\t0\r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "#the catalog file generated by cstacks \n",
    "!zcat batch_303.catalog.tags.tsv.gz | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [stacks manual](http://catchenlab.life.illinois.edu/stacks/manual/#cfiles), \n",
    "\n",
    "Col. 3 = Locus ID (python index = 2)\n",
    "\n",
    "Col. 10 = Sequence (python index = 8?? tried 9 and it didn't work)\n",
    "\n",
    "So I wrote a python script to extract the sequences for each of the unique loci produced by `populations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#move the python script and the loci list into stacks_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/stacks_base'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv populations/batch_303loci.txt stacks_base/batch_303loci.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv genBOWTIEfasta.py stacks_base/genBOWTIEfasta.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/stacks_base\n"
     ]
    }
   ],
   "source": [
    "cd stacks_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!gzip -d batch_303.catalog.tags.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python genBOWTIEfasta.py batch_303loci.txt batch_303.catalog.tags.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2\r\n",
      "TGCAGGTAGATTGGGTCGGGGGGGGGGGGTCCCAACCAAGAACCCTTGGTCATGTTGGGAGTCAAACACAATAATGACTAATAATCCGTCTGCATTATCCCATGATCTACTAACGGTGTGTACGCGGCGCGTGTGAATTTGT\r\n",
      ">6\r\n",
      "TGCAGGGGGAGGGAGAGCCGCAGAGGAGAGGCGCGGGAAGGAGGAGTCAACGGAAGGAAGCGGGCTTAGAGCGTGCTCCCCTGTAGTCTGACGTTTGAATGCATTGAGGAAGTGGCGATCGTGTCGAAGATTAAAGAAGAGA\r\n",
      ">8\r\n",
      "TGCAGGAGGCTGAGTACCTGGGCGACACCGTCGGGGGTGGGGGGGGGATCCGTGAAACCCGAGATGTAGAAAGTTGAGGTGATACGGAACTGGCCACGCCCTCACGAAGAAACAGGTACGCGCATTTGTGGGGCTGACCAGC\r\n",
      ">10\r\n",
      "TGCAGGAGGACTAAAGGGGTTACAACTCTGTACCTTTCAGCTGGGTGTCGAACACCCTCACCACTCCTCATCTGAATGAGTGGAACTGAATGGGACTTGATGCTATCCCCTCATCGTGGCAAAAATACACTTGAAAAACGGT\r\n",
      ">11\r\n",
      "TGCAGGGGGAGGAGGAGCAGCAGCGCCCGCAGGTCCTGAGGGGGAGGGGCGGAACAGAGGATCAGGGCTAATGAAGTGGGTCAGGAAACCTCAGGAGCTGATGATCTGCCATGTAAACATCCAACGTAACAGAAGATGGTCC\r\n"
     ]
    }
   ],
   "source": [
    "!head seqsforBOWTIE.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir bowtie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv stacks_base/seqsforBOWTIE.fa bowtie/batch303_seqsforBOWTIE.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv genBOWTIEfasta.py ../populations/genBOWTIEfasta.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv batch_303loci.txt ../populations/batch_303loci.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/stacks_base\n"
     ]
    }
   ],
   "source": [
    "cd stacks_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gzip batch_303.catalog.tags.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to download the latest release of [BOWTIE](http://bowtie-bio.sourceforge.net/index.shtml) ([v. 1.1.2 - 6/23/2015](https://sourceforge.net/projects/bowtie-bio/files/bowtie/1.1.2/))\n",
    "\n",
    "Tried to make it universally available, but for now will jus work out of \"bowtie\" folder in UCstacksL1. had to `sudo apt install bowtie` from within the \"bowtie\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/stacks_base'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/bowtie\n"
     ]
    }
   ],
   "source": [
    "cd ../bowtie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mbatch303_seqsforBOWTIE.fa\u001b[0m*  \u001b[34;42mbowtie-1.1.2\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step Two:** index a reference fasta file that is the \"genome\" for the purposes of this alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!bowtie-build batch303_seqsforBOWTIE.fa batch303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mbatch303.1.ebwt\u001b[0m*  \u001b[01;32mbatch303.4.ebwt\u001b[0m*      \u001b[01;32mbatch303_seqsforBOWTIE.fa\u001b[0m*\r\n",
      "\u001b[01;32mbatch303.2.ebwt\u001b[0m*  \u001b[01;32mbatch303.rev.1.ebwt\u001b[0m*  \u001b[34;42mbowtie-1.1.2\u001b[0m/\r\n",
      "\u001b[01;32mbatch303.3.ebwt\u001b[0m*  \u001b[01;32mbatch303.rev.2.ebwt\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step Three:** Use bowtie to align the fasta file containing the genepop unique sequences to this new \"genome\"\n",
    "\n",
    "NOTES from Eleni's notebook:\n",
    "\n",
    "-f\n",
    "\n",
    "The query input files (specified either as m1 and m2, or as s) are FASTA files (usually having extension .fa, .mfa, .fna or similar). All quality values are assumed to be 40 on the Phred quality scale.\n",
    "\n",
    "-v `int`\n",
    "\n",
    "Report alignments with at most `int` mismatches. -e and -l options are ignored and quality values have no effect on what alignments are valid. -v is mutually exclusive with -n\n",
    "\n",
    "S/--sam\n",
    "\n",
    "Print alignments in SAM format. See the SAM output section of the manual for details. To suppress all SAM headers, use --sam-nohead in addition to -S/--sam. To suppress just the @SQ headers (e.g. if the alignment is against a very large number of reference sequences), use --sam-nosq in addition to -S/--sam. bowtie does not write BAM files directly, but SAM output can be converted to BAM on the fly by piping bowtie's output to samtools view. -S/--sam is not compatible with --refout.\n",
    "\n",
    "--sam-nohead\n",
    "\n",
    "Suppress header lines (starting with @) when output is -S/--sam. This must be specified in addition to -S/--sam. --sam-nohead is ignored unless -S/--sam is also specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# reads processed: 18720\r\n",
      "# reads with at least one reported alignment: 18720 (100.00%)\r\n",
      "# reads that failed to align: 0 (0.00%)\r\n",
      "Reported 18720 alignments to 1 output stream(s)\r\n"
     ]
    }
   ],
   "source": [
    "!bowtie -f -v 3 --sam --sam-nohead \\\n",
    "batch303 \\\n",
    "batch303_seqsforBOWTIE.fa \\\n",
    "batch303_bowtieOut.sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step Four:** Filter out the sequences that aligned to sequences that were not themselves. I did this using a script written by Dan Drinan, shared through Eleni's notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mbatch303.1.ebwt\u001b[0m*  \u001b[01;32mbatch303_bowtieOut.sam\u001b[0m*     \u001b[34;42mbowtie-1.1.2\u001b[0m/\r\n",
      "\u001b[01;32mbatch303.2.ebwt\u001b[0m*  \u001b[01;32mbatch303.rev.1.ebwt\u001b[0m*        \u001b[01;32mparseBowtie_DD.py\u001b[0m*\r\n",
      "\u001b[01;32mbatch303.3.ebwt\u001b[0m*  \u001b[01;32mbatch303.rev.2.ebwt\u001b[0m*\r\n",
      "\u001b[01;32mbatch303.4.ebwt\u001b[0m*  \u001b[01;32mbatch303_seqsforBOWTIE.fa\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# script written by Dan Drinan\r\n",
      "# cd /mnt/hgfs/D/sequencing_data/Herring_MarrowBleachCombo/batch5/reference_genome/bowtie\r\n",
      "# python parseBowtie.py batch_5_bowtieOutput.sam batch_5_bowtieOutput_filtered.fa\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#script is called parseBowtie_DD.py\n",
    "!head -n 5 parseBowtie_DD.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Bowtie output lines read: 18720\n",
      "Number of sequences written to output: 18720\n"
     ]
    }
   ],
   "source": [
    "!python parseBowtie_DD.py batch303_bowtieOut.sam batch303_bowtieOut_filtered.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, there should have been fewer sequences written to output than there were read. But Eleni's notebook also shows that for her batch, the script did not filter out loci. This means that there are no two loci that are similar enough to align to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 11/3/2016\n",
    "\n",
    "I need to repeat the above for each of the remaining batches. \n",
    "<br>\n",
    "<br>\n",
    "**Step One (1/2):** unzip all of the catalog.tags.tsv.gz files for the last two batches: \n",
    "330,\n",
    "1030\n",
    "\n",
    "To do this, I created a python script that writes *and* automatically calls a shell script. The input file is a tab-delimited list of the batch #s and the file folders where the catalog files can be found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/mf-fish546-2016/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\tstacks_m3\r\n",
      "1030\tstacks_m10\r\n"
     ]
    }
   ],
   "source": [
    "#what does the input file look like? \n",
    "!head bowtie_BatchFolders.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### create a shell script that unzips catalog.tags.tsv.gz files ##########\r\n",
      "\r\n",
      "### in the command line, start from the `DataAnalysis/scripts/` folder\r\n",
      "import sys \r\n",
      "inputfile = open(sys.argv[1], \"r\")\r\n",
      "batch_list = []\r\n",
      "catfolder_list = []\r\n",
      "\r\n",
      "\r\n",
      "# create a list of the batch numbers, and the associated folders for those batches' catalogs\r\n"
     ]
    }
   ],
   "source": [
    "#what does the python scripts look like? \n",
    "!head bowtie_genShellpart1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches for bowtie:  ['330', '1030']\r\n",
      "batches found in folders:  ['stacks_m3', 'stacks_m10']\r\n"
     ]
    }
   ],
   "source": [
    "#run the python script\n",
    "!python bowtie_genShellpart1.py bowtie_BatchFolders.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd ../UCstacksL1\r\n",
      "\r\n",
      "cd stacks_m3\r\n",
      "gzip -d batch_330.catalog.tags.tsv.gz\r\n",
      "cd ../\r\n",
      "cd stacks_m10\r\n",
      "gzip -d batch_1030.catalog.tags.tsv.gz\r\n",
      "cd ../\r\n"
     ]
    }
   ],
   "source": [
    "#what is in the output shell script? \n",
    "!head -n 10 BOWTIEshellp1.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/stacks_m10\n"
     ]
    }
   ],
   "source": [
    "cd ../UCstacksL1/stacks_m10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cstacks version 1.42; catalog generated on 2016-10-22 06:55:53\r\n",
      "0\t1030\t1\t\t0\t+\tconsensus\t0\t16_117\tTGCAGGACCAGAGGGCGGGACCAGAGGGCTGGACCGGAGGTCGGGACCAGAGGGCGGGACCAGAGGGCGGGACTAGGGCCCTGGGGACAATGGGTCCTCTCAGTATTTAACCAGATGGGACTTCTCAGGTGAATCTATAATT\t0\t0\t0\t0\r\n"
     ]
    }
   ],
   "source": [
    "#did it work?\n",
    "!head -n 2 batch_1030.catalog.tags.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step One (2/2):** Create a .fasta file from the two catalogs that includes all of the loci listed in the `populations` genepop folder. \n",
    "\n",
    "\n",
    "I've already manually created the batch_330loci.txt and batch_1030loci.txt files. \n",
    "\n",
    "Instead of moving around the script / loci files as I did in the first round, I'm just including the relative paths. I tried to alter the script so that it uses the `subprocess.call` command to automatically move the new .fa file into the \"bowtie/\" folder, and rename it according to the batch, but that doesn't work in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batch 330\n",
    "!python genBOWTIEfasta.py ../UCstacksL1/populations/batch_330loci.txt ../UCstacksL1/stacks_m3/batch_330.catalog.tags.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv seqsforBOWTIE.fa ../UCstacksL1/bowtie/batch330_seqsforBOWTIE.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#batch 1030\n",
    "!python genBOWTIEfasta.py ../UCstacksL1/populations/batch_1030loci.txt ../UCstacksL1/stacks_m10/batch_1030.catalog.tags.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv seqsforBOWTIE.fa ../UCstacksL1/bowtie/batch1030_seqsforBOWTIE.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/bowtie\n"
     ]
    }
   ],
   "source": [
    "cd ../UCstacksL1/bowtie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">4\r\n",
      "TGCAGGGAGCAGACCGCGGGGGGAATAAGGTCGGCTCTTTGAGCTCACTGGTCACAAGGGGAACCCTATGGCCTCTTCCTTTTCCTTTAAATTAAGCCCTTCTTTATCTGATTCCCCAGACAGCCAGCGAGCAGACAGAGAG\r\n",
      ">11\r\n",
      "TGCAGGCTCTGCTTGAGGAGACTGATCTTTCTGGTCAACTCCTGATTGCTGCTATGTGCCTCCTGTAGCCTGAGGTCGTCCTCTCCACTCCTCTGGACCAGGGTCCTCAGCTGCACCTCTATCTCGCTACGCCTCCTAACCT\r\n",
      ">13\r\n",
      "TGCAGGTGGATCACTATCAAAGCGACAGTAGAACACATTTAGCTCATTTGCAAGGCGCAGGTTGTGGATGTAGCGAGGGGGGGTTGGTTTATAGTTGGTGATCTGCTTGAGTCCCTTCCAAACAGATCTAGAGTCTTTGGCT\r\n",
      ">17\r\n",
      "TGCAGGGTGTGGTTCGGTTCGCCTCAGTCCGGTAGGGAGGGGGCGGTATAGCCCAGCTCAGCTCCGAGGTCGCGTTTCCACCGCCGACAGTACCCTTTATGGTAGGCCGCATGTCGAATCGCCGCGGCAGCTACGTAAACAT\r\n",
      ">23\r\n",
      "TGCAGGTTCTCCTGTCCAGTCCCTATTGGAACATGCTGTCGTGCAAGACACTCCTAGCTGCAGCTCTACTACTCTGCTGCTGTGAGTAGGAATACGTCTCCACTCTGACACAGACCATTATCCTCAGAGTCAAGGCACAAGG\r\n"
     ]
    }
   ],
   "source": [
    "!head batch330_seqsforBOWTIE.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2\r\n",
      "TGCAGGACTCATAGTGCTTGCTCACTTTAAGAAATACTAGAACACCACAGCCCCTGATTAGTACAGTGGTCAGTATCCATTGGCCATGCATGAAACGAGGAATGTGAAGATTCTTGGTTTAACTCAGTCCGAACGAAAAGAG\r\n",
      ">3\r\n",
      "TGCAGGAGTACTACGCTCGAAAAGACGAACAGGAAGCCAATGCGGTAGGTCACTTCCTGTTGTTCCTGACTGCAGCCAATCAGATATCAGGAAGTACATGTTACTGAATGGTAACATTATTTATAAATCAATTATGGAGCTC\r\n",
      ">4\r\n",
      "TGCAGGAGAGAGTGAAGGGAGCTCTTGTCAGGTCTCGCTTCCTCCAGCTAAAGGATATGGATGCCCCAAGCTCTTTCTTCTTCAATCTGGAGAGATCGGTGGCACAGAGGAAGCAGATGACCTGCCTTAAGCTTCCTGATGG\r\n",
      ">5\r\n",
      "TGCAGGAGAGAGTGAAGGGAGCTCTTGTCAGGTCTCGCTTCCTCCAGCTAAAGGATATGGATGCCCCAAGCTCTTTCTTCTTCAATCTGGAGAGATCGGTGGCACAGAGGAAGCAGATGACCTGCCTTAAGCTTCCTGGAGG\r\n",
      ">7\r\n",
      "TGCAGGAGAGAGTGAAGGGAGCTCTTGTCAGGTCTCGCTTCCTCCAGCTGAAGGATATGGATGCCCCAAGCTCTTTCTTTTTTAATCTGGAGAGATCGGTGGCACAGAGGAAGCAGATGACCTGCCTTAAGCTTCCTGGAGG\r\n"
     ]
    }
   ],
   "source": [
    "!head batch1030_seqsforBOWTIE.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step Two:** index a reference fasta file that is the \"genome\" for the purposes of this alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      "  Output files: \"batch330.*.ebwt\"\n",
      "  Line rate: 6 (line is 64 bytes)\n",
      "  Lines per side: 1 (side is 64 bytes)\n",
      "  Offset rate: 5 (one in 32)\n",
      "  FTable chars: 10\n",
      "  Strings: unpacked\n",
      "  Max bucket size: default\n",
      "  Max bucket size, sqrt multiplier: default\n",
      "  Max bucket size, len divisor: 4\n",
      "  Difference-cover sample period: 1024\n",
      "  Endianness: little\n",
      "  Actual local endianness: little\n",
      "  Sanity checking: disabled\n",
      "  Assertions: disabled\n",
      "  Random seed: 0\n",
      "  Sizeofs: void*:8, int:4, long:8, size_t:8\n",
      "Input files DNA, FASTA:\n",
      "  batch330_seqsforBOWTIE.fa\n",
      "Reading reference sizes\n",
      "  Time reading reference sizes: 00:00:00\n",
      "Calculating joined length\n",
      "Writing header\n",
      "Reserving space for joined string\n",
      "Joining reference sequences\n",
      "  Time to join reference sequences: 00:00:00\n",
      "bmax according to bmaxDivN setting: 669707\n",
      "Using parameters --bmax 502281 --dcv 1024\n",
      "  Doing ahead-of-time memory usage test\n",
      "  Passed!  Constructing with these parameters: --bmax 502281 --dcv 1024\n",
      "Constructing suffix-array element generator\n",
      "Building DifferenceCoverSample\n",
      "  Building sPrime\n",
      "  Building sPrimeOrder\n",
      "  V-Sorting samples\n",
      "  V-Sorting samples time: 00:00:00\n",
      "  Allocating rank array\n",
      "  Ranking v-sort output\n",
      "  Ranking v-sort output time: 00:00:00\n",
      "  Invoking Larsson-Sadakane on ranks\n",
      "  Invoking Larsson-Sadakane on ranks time: 00:00:00\n",
      "  Sanity-checking and returning\n",
      "Building samples\n",
      "Reserving space for 12 sample suffixes\n",
      "Generating random suffixes\n",
      "QSorting 12 sample offsets, eliminating duplicates\n",
      "QSorting sample offsets, eliminating duplicates time: 00:00:00\n",
      "Multikey QSorting 12 samples\n",
      "  (Using difference cover)\n",
      "  Multikey QSorting samples time: 00:00:00\n",
      "Calculating bucket sizes\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Split 2, merged 6; iterating...\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Avg bucket size: 334853 (target: 502280)\n",
      "Converting suffix-array elements to index image\n",
      "Allocating ftab, absorbFtab\n",
      "Entering Ebwt loop\n",
      "Getting block 1 of 8\n",
      "  Reserving size (502281) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 405354\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 405355\n",
      "Getting block 2 of 8\n",
      "  Reserving size (502281) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 169344\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 169345\n",
      "Getting block 3 of 8\n",
      "  Reserving size (502281) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 404612\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 404613\n",
      "Getting block 4 of 8\n",
      "  Reserving size (502281) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 117778\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 117779\n",
      "Getting block 5 of 8\n",
      "  Reserving size (502281) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 473870\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 473871\n",
      "Getting block 6 of 8\n",
      "  Reserving size (502281) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 231468\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 231469\n",
      "Getting block 7 of 8\n",
      "  Reserving size (502281) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 413647\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 413648\n",
      "Getting block 8 of 8\n",
      "  Reserving size (502281) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:01\n",
      "  Sorting block of length 462750\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 462751\n",
      "Exited Ebwt loop\n",
      "fchr[A]: 0\n",
      "fchr[C]: 655097\n",
      "fchr[G]: 1334006\n",
      "fchr[T]: 2063207\n",
      "fchr[$]: 2678830\n",
      "Exiting Ebwt::buildToDisk()\n",
      "Returning from initFromVector\n",
      "Wrote 5370630 bytes to primary EBWT file: batch330.1.ebwt\n",
      "Wrote 334860 bytes to secondary EBWT file: batch330.2.ebwt\n",
      "Re-opening _in1 and _in2 as input streams\n",
      "Returning from Ebwt constructor\n",
      "Headers:\n",
      "    len: 2678830\n",
      "    bwtLen: 2678831\n",
      "    sz: 669708\n",
      "    bwtSz: 669708\n",
      "    lineRate: 6\n",
      "    linesPerSide: 1\n",
      "    offRate: 5\n",
      "    offMask: 0xffffffe0\n",
      "    isaRate: -1\n",
      "    isaMask: 0xffffffff\n",
      "    ftabChars: 10\n",
      "    eftabLen: 20\n",
      "    eftabSz: 80\n",
      "    ftabLen: 1048577\n",
      "    ftabSz: 4194308\n",
      "    offsLen: 83714\n",
      "    offsSz: 334856\n",
      "    isaLen: 0\n",
      "    isaSz: 0\n",
      "    lineSz: 64\n",
      "    sideSz: 64\n",
      "    sideBwtSz: 56\n",
      "    sideBwtLen: 224\n",
      "    numSidePairs: 5980\n",
      "    numSides: 11960\n",
      "    numLines: 11960\n",
      "    ebwtTotLen: 765440\n",
      "    ebwtTotSz: 765440\n",
      "    reverse: 0\n",
      "Total time for call to driver() for forward index: 00:00:04\n",
      "Reading reference sizes\n",
      "  Time reading reference sizes: 00:00:00\n",
      "Calculating joined length\n",
      "Writing header\n",
      "Reserving space for joined string\n",
      "Joining reference sequences\n",
      "  Time to join reference sequences: 00:00:00\n",
      "bmax according to bmaxDivN setting: 669707\n",
      "Using parameters --bmax 502281 --dcv 1024\n",
      "  Doing ahead-of-time memory usage test\n",
      "  Passed!  Constructing with these parameters: --bmax 502281 --dcv 1024\n",
      "Constructing suffix-array element generator\n",
      "Building DifferenceCoverSample\n",
      "  Building sPrime\n",
      "  Building sPrimeOrder\n",
      "  V-Sorting samples\n",
      "  V-Sorting samples time: 00:00:00\n",
      "  Allocating rank array\n",
      "  Ranking v-sort output\n",
      "  Ranking v-sort output time: 00:00:00\n",
      "  Invoking Larsson-Sadakane on ranks\n",
      "  Invoking Larsson-Sadakane on ranks time: 00:00:00\n",
      "  Sanity-checking and returning\n",
      "Building samples\n",
      "Reserving space for 12 sample suffixes\n",
      "Generating random suffixes\n",
      "QSorting 12 sample offsets, eliminating duplicates\n",
      "QSorting sample offsets, eliminating duplicates time: 00:00:00\n",
      "Multikey QSorting 12 samples\n",
      "  (Using difference cover)\n",
      "  Multikey QSorting samples time: 00:00:00\n",
      "Calculating bucket sizes\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Split 1, merged 6; iterating...\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Avg bucket size: 382689 (target: 502280)\n",
      "Converting suffix-array elements to index image\n",
      "Allocating ftab, absorbFtab\n",
      "Entering Ebwt loop\n",
      "Getting block 1 of 7\n",
      "  Reserving size (502281) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 428377\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 428378\n",
      "Getting block 2 of 7\n",
      "  Reserving size (502281) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 496423\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 496424\n",
      "Getting block 3 of 7\n",
      "  Reserving size (502281) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 131862\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 131863\n",
      "Getting block 4 of 7\n",
      "  Reserving size (502281) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 487498\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 487499\n",
      "Getting block 5 of 7\n",
      "  Reserving size (502281) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 251554\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 251555\n",
      "Getting block 6 of 7\n",
      "  Reserving size (502281) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 485074\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 485075\n",
      "Getting block 7 of 7\n",
      "  Reserving size (502281) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 398036\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 398037\n",
      "Exited Ebwt loop\n",
      "fchr[A]: 0\n",
      "fchr[C]: 655097\n",
      "fchr[G]: 1334006\n",
      "fchr[T]: 2063207\n",
      "fchr[$]: 2678830\n",
      "Exiting Ebwt::buildToDisk()\n",
      "Returning from initFromVector\n",
      "Wrote 5370630 bytes to primary EBWT file: batch330.rev.1.ebwt\n",
      "Wrote 334860 bytes to secondary EBWT file: batch330.rev.2.ebwt\n",
      "Re-opening _in1 and _in2 as input streams\n",
      "Returning from Ebwt constructor\n",
      "Headers:\n",
      "    len: 2678830\n",
      "    bwtLen: 2678831\n",
      "    sz: 669708\n",
      "    bwtSz: 669708\n",
      "    lineRate: 6\n",
      "    linesPerSide: 1\n",
      "    offRate: 5\n",
      "    offMask: 0xffffffe0\n",
      "    isaRate: -1\n",
      "    isaMask: 0xffffffff\n",
      "    ftabChars: 10\n",
      "    eftabLen: 20\n",
      "    eftabSz: 80\n",
      "    ftabLen: 1048577\n",
      "    ftabSz: 4194308\n",
      "    offsLen: 83714\n",
      "    offsSz: 334856\n",
      "    isaLen: 0\n",
      "    isaSz: 0\n",
      "    lineSz: 64\n",
      "    sideSz: 64\n",
      "    sideBwtSz: 56\n",
      "    sideBwtLen: 224\n",
      "    numSidePairs: 5980\n",
      "    numSides: 11960\n",
      "    numLines: 11960\n",
      "    ebwtTotLen: 765440\n",
      "    ebwtTotSz: 765440\n",
      "    reverse: 0\n",
      "Total time for backward call to driver() for mirror index: 00:00:03\n"
     ]
    }
   ],
   "source": [
    "!bowtie-build batch330_seqsforBOWTIE.fa batch330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      "  Output files: \"batch1030.*.ebwt\"\n",
      "  Line rate: 6 (line is 64 bytes)\n",
      "  Lines per side: 1 (side is 64 bytes)\n",
      "  Offset rate: 5 (one in 32)\n",
      "  FTable chars: 10\n",
      "  Strings: unpacked\n",
      "  Max bucket size: default\n",
      "  Max bucket size, sqrt multiplier: default\n",
      "  Max bucket size, len divisor: 4\n",
      "  Difference-cover sample period: 1024\n",
      "  Endianness: little\n",
      "  Actual local endianness: little\n",
      "  Sanity checking: disabled\n",
      "  Assertions: disabled\n",
      "  Random seed: 0\n",
      "  Sizeofs: void*:8, int:4, long:8, size_t:8\n",
      "Input files DNA, FASTA:\n",
      "  batch1030_seqsforBOWTIE.fa\n",
      "Reading reference sizes\n",
      "  Time reading reference sizes: 00:00:01\n",
      "Calculating joined length\n",
      "Writing header\n",
      "Reserving space for joined string\n",
      "Joining reference sequences\n",
      "  Time to join reference sequences: 00:00:00\n",
      "bmax according to bmaxDivN setting: 623557\n",
      "Using parameters --bmax 467668 --dcv 1024\n",
      "  Doing ahead-of-time memory usage test\n",
      "  Passed!  Constructing with these parameters: --bmax 467668 --dcv 1024\n",
      "Constructing suffix-array element generator\n",
      "Building DifferenceCoverSample\n",
      "  Building sPrime\n",
      "  Building sPrimeOrder\n",
      "  V-Sorting samples\n",
      "  V-Sorting samples time: 00:00:00\n",
      "  Allocating rank array\n",
      "  Ranking v-sort output\n",
      "  Ranking v-sort output time: 00:00:00\n",
      "  Invoking Larsson-Sadakane on ranks\n",
      "  Invoking Larsson-Sadakane on ranks time: 00:00:00\n",
      "  Sanity-checking and returning\n",
      "Building samples\n",
      "Reserving space for 12 sample suffixes\n",
      "Generating random suffixes\n",
      "QSorting 12 sample offsets, eliminating duplicates\n",
      "QSorting sample offsets, eliminating duplicates time: 00:00:00\n",
      "Multikey QSorting 12 samples\n",
      "  (Using difference cover)\n",
      "  Multikey QSorting samples time: 00:00:00\n",
      "Calculating bucket sizes\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Split 1, merged 6; iterating...\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Avg bucket size: 356318 (target: 467667)\n",
      "Converting suffix-array elements to index image\n",
      "Allocating ftab, absorbFtab\n",
      "Entering Ebwt loop\n",
      "Getting block 1 of 7\n",
      "  Reserving size (467668) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 465298\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 465299\n",
      "Getting block 2 of 7\n",
      "  Reserving size (467668) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 252520\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 252521\n",
      "Getting block 3 of 7\n",
      "  Reserving size (467668) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 458381\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 458382\n",
      "Getting block 4 of 7\n",
      "  Reserving size (467668) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 192661\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 192662\n",
      "Getting block 5 of 7\n",
      "  Reserving size (467668) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 430129\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 430130\n",
      "Getting block 6 of 7\n",
      "  Reserving size (467668) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 447621\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 447622\n",
      "Getting block 7 of 7\n",
      "  Reserving size (467668) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 247614\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 247615\n",
      "Exited Ebwt loop\n",
      "fchr[A]: 0\n",
      "fchr[C]: 612482\n",
      "fchr[G]: 1239455\n",
      "fchr[T]: 1916240\n",
      "fchr[$]: 2494230\n",
      "Exiting Ebwt::buildToDisk()\n",
      "Returning from initFromVector\n",
      "Wrote 5288302 bytes to primary EBWT file: batch1030.1.ebwt\n",
      "Wrote 311784 bytes to secondary EBWT file: batch1030.2.ebwt\n",
      "Re-opening _in1 and _in2 as input streams\n",
      "Returning from Ebwt constructor\n",
      "Headers:\n",
      "    len: 2494230\n",
      "    bwtLen: 2494231\n",
      "    sz: 623558\n",
      "    bwtSz: 623558\n",
      "    lineRate: 6\n",
      "    linesPerSide: 1\n",
      "    offRate: 5\n",
      "    offMask: 0xffffffe0\n",
      "    isaRate: -1\n",
      "    isaMask: 0xffffffff\n",
      "    ftabChars: 10\n",
      "    eftabLen: 20\n",
      "    eftabSz: 80\n",
      "    ftabLen: 1048577\n",
      "    ftabSz: 4194308\n",
      "    offsLen: 77945\n",
      "    offsSz: 311780\n",
      "    isaLen: 0\n",
      "    isaSz: 0\n",
      "    lineSz: 64\n",
      "    sideSz: 64\n",
      "    sideBwtSz: 56\n",
      "    sideBwtLen: 224\n",
      "    numSidePairs: 5568\n",
      "    numSides: 11136\n",
      "    numLines: 11136\n",
      "    ebwtTotLen: 712704\n",
      "    ebwtTotSz: 712704\n",
      "    reverse: 0\n",
      "Total time for call to driver() for forward index: 00:00:03\n",
      "Reading reference sizes\n",
      "  Time reading reference sizes: 00:00:00\n",
      "Calculating joined length\n",
      "Writing header\n",
      "Reserving space for joined string\n",
      "Joining reference sequences\n",
      "  Time to join reference sequences: 00:00:00\n",
      "bmax according to bmaxDivN setting: 623557\n",
      "Using parameters --bmax 467668 --dcv 1024\n",
      "  Doing ahead-of-time memory usage test\n",
      "  Passed!  Constructing with these parameters: --bmax 467668 --dcv 1024\n",
      "Constructing suffix-array element generator\n",
      "Building DifferenceCoverSample\n",
      "  Building sPrime\n",
      "  Building sPrimeOrder\n",
      "  V-Sorting samples\n",
      "  V-Sorting samples time: 00:00:00\n",
      "  Allocating rank array\n",
      "  Ranking v-sort output\n",
      "  Ranking v-sort output time: 00:00:00\n",
      "  Invoking Larsson-Sadakane on ranks\n",
      "  Invoking Larsson-Sadakane on ranks time: 00:00:00\n",
      "  Sanity-checking and returning\n",
      "Building samples\n",
      "Reserving space for 12 sample suffixes\n",
      "Generating random suffixes\n",
      "QSorting 12 sample offsets, eliminating duplicates\n",
      "QSorting sample offsets, eliminating duplicates time: 00:00:00\n",
      "Multikey QSorting 12 samples\n",
      "  (Using difference cover)\n",
      "  Multikey QSorting samples time: 00:00:00\n",
      "Calculating bucket sizes\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:01\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Split 1, merged 6; iterating...\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Avg bucket size: 311778 (target: 467667)\n",
      "Converting suffix-array elements to index image\n",
      "Allocating ftab, absorbFtab\n",
      "Entering Ebwt loop\n",
      "Getting block 1 of 8\n",
      "  Reserving size (467668) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 206874\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 206875\n",
      "Getting block 2 of 8\n",
      "  Reserving size (467668) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 319394\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 319395\n",
      "Getting block 3 of 8\n",
      "  Reserving size (467668) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 396938\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 396939\n",
      "Getting block 4 of 8\n",
      "  Reserving size (467668) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 412553\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 412554\n",
      "Getting block 5 of 8\n",
      "  Reserving size (467668) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 294705\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 294706\n",
      "Getting block 6 of 8\n",
      "  Reserving size (467668) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 285770\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 285771\n",
      "Getting block 7 of 8\n",
      "  Reserving size (467668) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 361423\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 361424\n",
      "Getting block 8 of 8\n",
      "  Reserving size (467668) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 216566\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 216567\n",
      "Exited Ebwt loop\n",
      "fchr[A]: 0\n",
      "fchr[C]: 612482\n",
      "fchr[G]: 1239455\n",
      "fchr[T]: 1916240\n",
      "fchr[$]: 2494230\n",
      "Exiting Ebwt::buildToDisk()\n",
      "Returning from initFromVector\n",
      "Wrote 5288302 bytes to primary EBWT file: batch1030.rev.1.ebwt\n",
      "Wrote 311784 bytes to secondary EBWT file: batch1030.rev.2.ebwt\n",
      "Re-opening _in1 and _in2 as input streams\n",
      "Returning from Ebwt constructor\n",
      "Headers:\n",
      "    len: 2494230\n",
      "    bwtLen: 2494231\n",
      "    sz: 623558\n",
      "    bwtSz: 623558\n",
      "    lineRate: 6\n",
      "    linesPerSide: 1\n",
      "    offRate: 5\n",
      "    offMask: 0xffffffe0\n",
      "    isaRate: -1\n",
      "    isaMask: 0xffffffff\n",
      "    ftabChars: 10\n",
      "    eftabLen: 20\n",
      "    eftabSz: 80\n",
      "    ftabLen: 1048577\n",
      "    ftabSz: 4194308\n",
      "    offsLen: 77945\n",
      "    offsSz: 311780\n",
      "    isaLen: 0\n",
      "    isaSz: 0\n",
      "    lineSz: 64\n",
      "    sideSz: 64\n",
      "    sideBwtSz: 56\n",
      "    sideBwtLen: 224\n",
      "    numSidePairs: 5568\n",
      "    numSides: 11136\n",
      "    numLines: 11136\n",
      "    ebwtTotLen: 712704\n",
      "    ebwtTotSz: 712704\n",
      "    reverse: 0\n",
      "Total time for backward call to driver() for mirror index: 00:00:03\n"
     ]
    }
   ],
   "source": [
    "!bowtie-build batch1030_seqsforBOWTIE.fa batch1030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mbatch1030.1.ebwt\u001b[0m*                \u001b[01;32mbatch303.rev.1.ebwt\u001b[0m*\r\n",
      "\u001b[01;32mbatch1030.2.ebwt\u001b[0m*                \u001b[01;32mbatch303.rev.2.ebwt\u001b[0m*\r\n",
      "\u001b[01;32mbatch1030.3.ebwt\u001b[0m*                \u001b[01;32mbatch303_seqsforBOWTIE.fa\u001b[0m*\r\n",
      "\u001b[01;32mbatch1030.4.ebwt\u001b[0m*                \u001b[01;32mbatch330.1.ebwt\u001b[0m*\r\n",
      "\u001b[01;32mbatch1030.rev.1.ebwt\u001b[0m*            \u001b[01;32mbatch330.2.ebwt\u001b[0m*\r\n",
      "\u001b[01;32mbatch1030.rev.2.ebwt\u001b[0m*            \u001b[01;32mbatch330.3.ebwt\u001b[0m*\r\n",
      "\u001b[01;32mbatch1030_seqsforBOWTIE.fa\u001b[0m*      \u001b[01;32mbatch330.4.ebwt\u001b[0m*\r\n",
      "\u001b[01;32mbatch303.1.ebwt\u001b[0m*                 \u001b[01;32mbatch330.rev.1.ebwt\u001b[0m*\r\n",
      "\u001b[01;32mbatch303.2.ebwt\u001b[0m*                 \u001b[01;32mbatch330.rev.2.ebwt\u001b[0m*\r\n",
      "\u001b[01;32mbatch303.3.ebwt\u001b[0m*                 \u001b[01;32mbatch330_seqsforBOWTIE.fa\u001b[0m*\r\n",
      "\u001b[01;32mbatch303.4.ebwt\u001b[0m*                 \u001b[34;42mbowtie-1.1.2\u001b[0m/\r\n",
      "\u001b[01;32mbatch303_bowtieOut_filtered.fa\u001b[0m*  \u001b[01;32mparseBowtie_DD.py\u001b[0m*\r\n",
      "\u001b[01;32mbatch303_bowtieOut.sam\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step Three:** Use bowtie to align the fasta file containing the genepop unique sequences to this new \"genome\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# reads processed: 18865\n",
      "# reads with at least one reported alignment: 18865 (100.00%)\n",
      "# reads that failed to align: 0 (0.00%)\n",
      "Reported 18865 alignments to 1 output stream(s)\n",
      "# reads processed: 17565\n",
      "# reads with at least one reported alignment: 17565 (100.00%)\n",
      "# reads that failed to align: 0 (0.00%)\n",
      "Reported 17565 alignments to 1 output stream(s)\n"
     ]
    }
   ],
   "source": [
    "!bowtie -f -v 3 --sam --sam-nohead batch330 batch330_seqsforBOWTIE.fa batch330_bowtieOut.sam\n",
    "!bowtie -f -v 3 --sam --sam-nohead batch1030 batch1030_seqsforBOWTIE.fa batch1030_bowtieOut.sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step Four:** Filter out the sequences that aligned to sequences that were not themselves. I did this using a script written by Dan Drinan, shared through Eleni's notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Bowtie output lines read: 18865\n",
      "Number of sequences written to output: 18865\n"
     ]
    }
   ],
   "source": [
    "!python parseBowtie_DD.py batch330_bowtieOut.sam batch330_bowtieOut_filtered.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Bowtie output lines read: 17565\n",
      "Number of sequences written to output: 17565\n"
     ]
    }
   ],
   "source": [
    "!python parseBowtie_DD.py batch1030_bowtieOut.sam batch1030_bowtieOut_filtered.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) BLAST\n",
    "\n",
    "Now I'm running blast to again align the sequences to themselves, and will get rid of any sequence that aligns to more than itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir blast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/blast\n"
     ]
    }
   ],
   "source": [
    "cd blast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first want to move all of the filtered bowtie files into a new BLAST folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv ../bowtie/batch303_bowtieOut_filtered.fa batch303_bowtieOut_filtered.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv ../bowtie/batch330_bowtieOut_filtered.fa batch330_bowtieOut_filtered.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv ../bowtie/batch1030_bowtieOut_filtered.fa batch1030_bowtieOut_filtered.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm then going to make a database from each of these bowtie filtered files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Building a new DB, current time: 11/03/2016 14:38:11\n",
      "New DB name:   /mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/blast/batch303_bowtieFiltered\n",
      "New DB title:  batch303_bowtieOut_filtered.fa\n",
      "Sequence type: Nucleotide\n",
      "Keep Linkouts: T\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 18720 sequences in 0.343858 seconds.\n",
      "\n",
      "\n",
      "Building a new DB, current time: 11/03/2016 14:38:11\n",
      "New DB name:   /mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/blast/batch330_bowtieFiltered\n",
      "New DB title:  batch330_bowtieOut_filtered.fa\n",
      "Sequence type: Nucleotide\n",
      "Keep Linkouts: T\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 18865 sequences in 0.32121 seconds.\n",
      "\n",
      "\n",
      "Building a new DB, current time: 11/03/2016 14:38:12\n",
      "New DB name:   /mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/blast/batch1030_bowtieFiltered\n",
      "New DB title:  batch1030_bowtieOut_filtered.fa\n",
      "Sequence type: Nucleotide\n",
      "Keep Linkouts: T\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 17565 sequences in 0.38347 seconds.\n"
     ]
    }
   ],
   "source": [
    "!makeblastdb -in batch303_bowtieOut_filtered.fa \\\n",
    "-parse_seqids \\\n",
    "-dbtype nucl \\\n",
    "-out batch303_bowtieFiltered\n",
    "\n",
    "\n",
    "!makeblastdb -in batch330_bowtieOut_filtered.fa \\\n",
    "-parse_seqids \\\n",
    "-dbtype nucl \\\n",
    "-out batch330_bowtieFiltered\n",
    "\n",
    "!makeblastdb -in batch1030_bowtieOut_filtered.fa \\\n",
    "-parse_seqids \\\n",
    "-dbtype nucl \\\n",
    "-out batch1030_bowtieFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mbatch1030_bowtieFiltered.nhr\u001b[0m*     \u001b[01;32mbatch303_bowtieFiltered.nsi\u001b[0m*\r\n",
      "\u001b[01;32mbatch1030_bowtieFiltered.nin\u001b[0m*     \u001b[01;32mbatch303_bowtieFiltered.nsq\u001b[0m*\r\n",
      "\u001b[01;32mbatch1030_bowtieFiltered.nog\u001b[0m*     \u001b[01;32mbatch303_bowtieOut_filtered.fa\u001b[0m*\r\n",
      "\u001b[01;32mbatch1030_bowtieFiltered.nsd\u001b[0m*     \u001b[01;32mbatch330_bowtieFiltered.nhr\u001b[0m*\r\n",
      "\u001b[01;32mbatch1030_bowtieFiltered.nsi\u001b[0m*     \u001b[01;32mbatch330_bowtieFiltered.nin\u001b[0m*\r\n",
      "\u001b[01;32mbatch1030_bowtieFiltered.nsq\u001b[0m*     \u001b[01;32mbatch330_bowtieFiltered.nog\u001b[0m*\r\n",
      "\u001b[01;32mbatch1030_bowtieOut_filtered.fa\u001b[0m*  \u001b[01;32mbatch330_bowtieFiltered.nsd\u001b[0m*\r\n",
      "\u001b[01;32mbatch303_bowtieFiltered.nhr\u001b[0m*      \u001b[01;32mbatch330_bowtieFiltered.nsi\u001b[0m*\r\n",
      "\u001b[01;32mbatch303_bowtieFiltered.nin\u001b[0m*      \u001b[01;32mbatch330_bowtieFiltered.nsq\u001b[0m*\r\n",
      "\u001b[01;32mbatch303_bowtieFiltered.nog\u001b[0m*      \u001b[01;32mbatch330_bowtieOut_filtered.fa\u001b[0m*\r\n",
      "\u001b[01;32mbatch303_bowtieFiltered.nsd\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now I can BLAST the sequences against themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!blastn -query batch303_bowtieOut_filtered.fa \\\n",
    "-db batch303_bowtieFiltered \\\n",
    "-out batch303_bowtie_blastFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!blastn -query batch330_bowtieOut_filtered.fa \\\n",
    "-db batch330_bowtieFiltered \\\n",
    "-out batch330_bowtie_blastFiltered\n",
    "\n",
    "!blastn -query batch1030_bowtieOut_filtered.fa \\\n",
    "-db batch1030_bowtieFiltered \\\n",
    "-out batch1030_bowtie_blastFiltered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then used a script written by Dan Drinan (out of Eleni's nb) to filter out the sequences that aligned to other sequences that were not themselves (which should remove microsats and repetitive sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Script written by Dan Drinan April 2016\r\n",
      "# this script checks to see that each locus was best aligned to itself. \r\n",
      "# if it best (or equally) aligned to another locus, it will be removed.\r\n",
      "\r\n",
      "# directory: \r\n",
      "# DataAnalysis/scripts\r\n",
      "# files: [1]blast input file, [2]fasta file that represents the data after it went through bowtie and got filtered [3] name of fasta file to store good data [4] name of file to store bad data\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 7 checkBlastResults_DD.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identifying which loci are 'good' and 'bad' based on BLAST alignments...\n",
      "Writing 'good' and 'bad' loci to their respective files...\n"
     ]
    }
   ],
   "source": [
    "!python checkBlastResults_DD.py \\\n",
    "../UCstacksL1/blast/batch303_bowtie_blastFiltered \\\n",
    "../UCstacksL1/blast/batch303_bowtieOut_filtered.fa \\\n",
    "../UCstacksL1/blast/batch303_bowtie_blastFiltered_GOOD.fa \\\n",
    "../UCstacksL1/blast/batch303_bowtie_blastFiltered_BAD.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identifying which loci are 'good' and 'bad' based on BLAST alignments...\n",
      "Writing 'good' and 'bad' loci to their respective files...\n"
     ]
    }
   ],
   "source": [
    "!python checkBlastResults_DD.py \\\n",
    "../UCstacksL1/blast/batch330_bowtie_blastFiltered \\\n",
    "../UCstacksL1/blast/batch330_bowtieOut_filtered.fa \\\n",
    "../UCstacksL1/blast/batch330_bowtie_blastFiltered_GOOD.fa \\\n",
    "../UCstacksL1/blast/batch330_bowtie_blastFiltered_BAD.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identifying which loci are 'good' and 'bad' based on BLAST alignments...\n",
      "Writing 'good' and 'bad' loci to their respective files...\n"
     ]
    }
   ],
   "source": [
    "!python checkBlastResults_DD.py \\\n",
    "../UCstacksL1/blast/batch1030_bowtie_blastFiltered \\\n",
    "../UCstacksL1/blast/batch1030_bowtieOut_filtered.fa \\\n",
    "../UCstacksL1/blast/batch1030_bowtie_blastFiltered_GOOD.fa \\\n",
    "../UCstacksL1/blast/batch1030_bowtie_blastFiltered_BAD.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/UCstacksL1/blast\n"
     ]
    }
   ],
   "source": [
    "cd ../UCstacksL1/blast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To count the number of loci in each of the GOOD fasta files, use grep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18277\r\n"
     ]
    }
   ],
   "source": [
    "!grep \">\" batch303_bowtie_blastFiltered_GOOD.fa | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18405\r\n"
     ]
    }
   ],
   "source": [
    "!grep \">\" batch330_bowtie_blastFiltered_GOOD.fa | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17205\r\n"
     ]
    }
   ],
   "source": [
    "!grep \">\" batch1030_bowtie_blastFiltered_GOOD.fa | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means that the number of sequences kicked out were..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443\n",
      "460\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "!grep \">\" batch303_bowtie_blastFiltered_BAD.fa | wc -l\n",
    "!grep \">\" batch330_bowtie_blastFiltered_BAD.fa | wc -l\n",
    "!grep \">\" batch1030_bowtie_blastFiltered_BAD.fa | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions: Check out this [excel file](https://github.com/mfisher5/mf-fish546-2016/blob/master/UCstacksL1/ParameterOutputComparisons.xlsx) or the visuals in this [Evernote nb](http://www.evernote.com/l/AooTivFI6ihBbredJVCygp1QPdCiYZ0X6Fk/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
