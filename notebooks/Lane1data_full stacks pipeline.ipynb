{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lane 1 (PE 150) Data\n",
    "<br>\n",
    "<br>\n",
    "This notebook goes through the stacks pipeline, from `process_radtags` to `cstacks`, for the first lane of sequencing recieved for the Korean Pacific cod. \n",
    "Cstacks was run for *de novo* assembly of reads into a reference catalog of loci. Sstacks was then run *without* additional filtering / screening of the reference catalog, to become familiar with the process. For future analysis, the parameters used in ustacks and cstacks will have to be optimized, and additional screening will be done to create a reference genome (see project workflow [here](https://github.com/mfisher5/mf-fish546-PCod/blob/master/Diagrams/PopGen_Workflow.md)). \n",
    "<br>\n",
    "<br>\n",
    "The primary purpose of this notebook is to run `process_radtags` to clean and demultiplex the data, and become more familiar with other steps in the stacks pipeline. This includes writing python scripts to generate shell scripts for one or more steps in the stacks pipeline. \n",
    "<br>\n",
    "<br>\n",
    "This notebook was translated from Evernote; the original [Evernote notebook](http://www.evernote.com/l/Aop4Hv2efMJKarQTFQ6NtlVES2W7fhalyvQ/) contains original notes written while writing the program, and has the related scripts, with parameters exactly as they were run here, attached. Note that some python scripts listed here that are in my github [scripts/](https://github.com/mfisher5/mf-fish546-2016/tree/master/scripts) folder may have been altered slightly since these analyses were run, since they are reused each time I run that step in the pipeline. \n",
    "<br>\n",
    "<br>\n",
    "Programs: FastQC, [stacks v. 1.42](http://catchenlab.life.illinois.edu/stacks/)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "## process_radtags\n",
    "\n",
    "\n",
    "**Notes on the Data:**\n",
    "\n",
    "Paired end, 150bp\n",
    "\n",
    "2 raw data files: R1 = forward, R2 = reverse\n",
    "\n",
    "File path: D/Pacific\\ cod/DataAnalysis/raw_data/L1_PE150 (*in Github: mf-fish546-2016/raw_data/L1PE_150*)\n",
    "\n",
    "*[Flowcell Summary](https://github.com/mfisher5/mf-fish546-2016/blob/master/raw_data/FlowcellSummaries/Lane1_FlowcellSummary.png)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10/7/2016\n",
    "\n",
    "**Dealing with paired-end data**: From the stacks manual\n",
    "\n",
    "________________________________________________________________________________________________________________________________\n",
    "*If your data are paired-end, Illumina HiSeq data, in a directory called raw:*\n",
    "<br>\n",
    "<br>\n",
    "`~/raw% ls`\n",
    "<br>\n",
    "`lane4_NoIndex_L004_R1_001.fastq  lane4_NoIndex_L004_R1_009.fastq  lane4_NoIndex_L004_R2_005.fastq\n",
    "lane4_NoIndex_L004_R1_002.fastq  lane4_NoIndex_L004_R1_010.fastq  lane4_NoIndex_L004_R2_006.fastq`\n",
    "<br>\n",
    "<br>\n",
    "Then you simply add the **-P flag.** process_radtags understands the Illumina naming scheme and will figure out how to properly pair the files together:\n",
    "\n",
    "`% process_radtags -P -p ./raw/ -o ./samples/ -b ./barcodes/barcodes_lane4 -e sbfI -E phred33 -r -c -q`\n",
    "________________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "**Trim Length:** FastQC\n",
    "\n",
    "Opened the original raw data (not demultiplexed to individuals) files through FastQC. Fragment length = 151 bp. The sequence quality tends to get pretty low within the last few bp in the forward sequence, so I trimmed to 149. In the reverse sequence the quality gets low much early (about 120bases), but Dan said that the reverse sequence is usually of poorer quality.\n",
    "\n",
    "FastQC for forward and reverse sequences [download through github](https://github.com/mfisher5/mf-fish546-2016/tree/master/raw_data/L1_PE150/FastQC)\n",
    "\n",
    "Or for quick access to the report, see the embedded HTML file in [Evernote](http://www.evernote.com/l/Aop4Hv2efMJKarQTFQ6NtlVES2W7fhalyvQ/)\n",
    "\n",
    "\n",
    "\n",
    "### Running process_radtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/mf-fish546-2016/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "#navigate to DataAnalysis folder (in github, mf-fish546-2016)\n",
    "cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir samples\n",
    "!mkdir samplesT140\n",
    "!mkdir samplesT146\n",
    "!mkdir samplesT142"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**barcodes.txt** A barcodes file is needed for process_radtags. I used the barcodes file I sent in with the sequencing, which is a simple text file with a list of barcodes. For the sake of simplicity in running process_radtags, I saved the barcode file in the root directory `DataAnalysis`, or on Github `mf-fish546-2016`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAACGG\r",
      "\r\n",
      "GCCGTA\r",
      "\r\n",
      "ACTCTT\r",
      "\r\n",
      "TTCTAG\r",
      "\r\n",
      "ATTCCG\r",
      "\r\n",
      "CCGCAT\r",
      "\r\n",
      "CGAGGC\r",
      "\r\n",
      "CGCAGA\r",
      "\r\n",
      "GAGAGA\r",
      "\r\n",
      "GGGGCG\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head barcodesL1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Using `--filter_illumina`, trimmed to 149 bp (forgot about barcodes being trimmed off!! So this doesn't actually remove any bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_radtags -p raw_data/L1_PE150/ \\\n",
    "-P \\\n",
    "-i gzfastq -y gzfasta \\\n",
    "-o samples \\\n",
    "-b barcodesL1.txt \\\n",
    "-e sbfI -E phred33 \\\n",
    "-r -c -q --filter_illumina \\\n",
    "-t 149"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "532905602 total reads\n",
    "\n",
    "0 failed Illumina filtered reads;\n",
    "\n",
    "48761576 ambiguous barcode drops;\n",
    "\n",
    "248505677 low quality read drops;\n",
    "\n",
    "235638349 retained reads. (44%)\n",
    "\n",
    "**! Will not use filter_illumina; too strict on this data !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** no `--filter_illumina`, trimmed to 140 bp (remove last 5 bases) (151 - 6 (barcodes) - 5 (trim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_radtags -p raw_data/L1_PE150/ \\\n",
    "-P \\\n",
    "-i gzfastq -y gzfasta \\\n",
    "-o samplesT140 \\\n",
    "-b barcodesL1.txt \\\n",
    "-e sbfI -E phred33 \\\n",
    "-r -c -q -t 140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "532905602 total reads\n",
    "\n",
    "48761576 ambiguous barcode drops;\n",
    "\n",
    "6222291 low quality read drops;\n",
    "\n",
    "30459515 ambiguous RAD-Tag drops;\n",
    "\n",
    "447462220 retained reads. (83.97%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10/10/2016\n",
    "\n",
    "**(3)** no filter_illumina, trimmed to 146 bp (remove NO bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_radtags -p raw_data/L1_PE150/ \\\n",
    "-P \\\n",
    "-i gzfastq -y gzfasta \\\n",
    "-o samplesT146 \\\n",
    "-b barcodesL1.txt \\\n",
    "-e sbfI -E phred33 \\\n",
    "-r -c -q -t 146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "532905602 total reads\n",
    "\n",
    "48761576 ambiguous barcode drops;\n",
    "\n",
    "6222291 low quality read drops;\n",
    "\n",
    "30459515 ambiguous RAD-Tag drops;\n",
    "\n",
    "447462220 retained reads. (83.97%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)** no filter_illumina, trimmed to 142 bp (remove last 3 bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_radtags -p raw_data/L1_PE150/ \\\n",
    "-P \\\n",
    "-i gzfastq -y gzfasta \\\n",
    "-o samplesT142 \\\n",
    "-b barcodesL1.txt \\\n",
    "-e sbfI -E phred33 \\\n",
    "-r -c -q -t 142"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "532905602 total reads\n",
    "\n",
    "48761576 ambiguous barcode drops;\n",
    "\n",
    "6414716 low quality read drops;\n",
    "\n",
    "30459515 ambiguous RAD-Tag drops;\n",
    "\n",
    "447269795 retained reads. (83.93%)\n",
    "\n",
    "** ! Will be using this output for ustacks ! **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to look at the new files generated: \n",
    "cd samplesT142\n",
    "!ls | head 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../ #return to root directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing unneccessary files\n",
    "\n",
    "Since I won't be using the files produced by  **(1)** or **(2)** above, I deleted these from my computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Files\n",
    "\n",
    "process-radtags has named all of my files by barcode, but it is easier for me to refer to them by sample ID (so that I know which samples belong to which populations). \n",
    "\n",
    "\n",
    "**(1)** I started out with a .csv file that has all of the barcodes next to their sample IDs. In excel, I inserted a column before the barcodes then added \"mv\": \"L1_mv_barcodesTOsample.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv\tAAACGG\tPO010715_06\r",
      "\r\n",
      "mv\tGCCGTA\tPO010715_27\r",
      "\r\n",
      "mv\tACTCTT\tPO010715_28\r",
      "\r\n",
      "mv\tTTCTAG\tPO010715_29\r",
      "\r\n",
      "mv\tATTCCG\tGE011215_08\r",
      "\r\n",
      "mv\tCCGCAT\tGE011215_09\r",
      "\r\n",
      "mv\tCGAGGC\tGE011215_14\r",
      "\r\n",
      "mv\tCGCAGA\tGE011215_15\r",
      "\r\n",
      "mv\tGAGAGA\tNA021015_16\r",
      "\r\n",
      "mv\tGGGGCG\tNA021015_21\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head L1_mv_barcodesTOsample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** I ran a python script in the command line, to convert the list of barcodes / samples into a list of barcode / sample fasta files. The python script is specifically for paired end output. Paired end data has four \"sample\" fasta files output per individual: 1. forward sequences, 2. reverse sequences, 3. removed forward sequences from quality filtering, and 4. removed reverse sequences from quality filtering. Single read data has only (1) and (3). *I also have a script in the folder that handles filename conversions for single read data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python filenames_PEconversion.py L1_mv_barcodesTOsample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outputs two files named \"new_filenames1.txt\" and \"new_filenames2.txt\". Since I'm only going to be working with the forward sequences for the rest of the stacks pipeline, I'll only need `new_filenames1.txt` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv\tsample_AAACGG.1.fa.gz\tPO010715_06.1.fa.gz\r\n",
      "mv\tsample_GCCGTA.1.fa.gz\tPO010715_27.1.fa.gz\r\n",
      "mv\tsample_ACTCTT.1.fa.gz\tPO010715_28.1.fa.gz\r\n",
      "mv\tsample_TTCTAG.1.fa.gz\tPO010715_29.1.fa.gz\r\n",
      "mv\tsample_ATTCCG.1.fa.gz\tGE011215_08.1.fa.gz\r\n",
      "mv\tsample_CCGCAT.1.fa.gz\tGE011215_09.1.fa.gz\r\n",
      "mv\tsample_CGAGGC.1.fa.gz\tGE011215_14.1.fa.gz\r\n",
      "mv\tsample_CGCAGA.1.fa.gz\tGE011215_15.1.fa.gz\r\n",
      "mv\tsample_GAGAGA.1.fa.gz\tNA021015_16.1.fa.gz\r\n",
      "mv\tsample_GGGGCG.1.fa.gz\tNA021015_21.1.fa.gz\r\n"
     ]
    }
   ],
   "source": [
    "!mv new_filenames1.txt L1_filename_conversion.txt\n",
    "!rm new_filenames2.txt\n",
    "!head L1_filename_conversion.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** I copied and pasted the contents of the new `L1_filename_conversion.txt` file directly into the command line. This renames the files in *each of the samples folders* by sample ID, rather than by barcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd samplesT142\n",
    "#copy and paste commands from L1_filename_conversion\n",
    "cd ../samplesT146\n",
    "#copy and paste commands from L1_filename_conversion\n",
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/samplesT142\n"
     ]
    }
   ],
   "source": [
    "cd samplesT142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mGE011215_01.1.fa.gz\u001b[0m*\r\n",
      "\u001b[01;32mGE011215_07.1.fa.gz\u001b[0m*\r\n",
      "\u001b[01;32mGE011215_08.1.fa.gz\u001b[0m*\r\n",
      "\u001b[01;32mGE011215_09.1.fa.gz\u001b[0m*\r\n",
      "\u001b[01;32mGE011215_10.1.fa.gz\u001b[0m*\r\n",
      "\u001b[01;32mGE011215_14.1.fa.gz\u001b[0m*\r\n",
      "\u001b[01;32mGE011215_15.1.fa.gz\u001b[0m*\r\n",
      "\u001b[01;32mGE011215_16.1.fa.gz\u001b[0m*\r\n",
      "\u001b[01;32mGE011215_20.1.fa.gz\u001b[0m*\r\n",
      "\u001b[01;32mGE011215_21.1.fa.gz\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10/11/2016\n",
    "\n",
    "### Running ustacks\n",
    "\n",
    "**! For now, I am only running ustacks on the forward scripts !**\n",
    "\n",
    "**(1)** Using the `ustacks_replaceIDs.py` script, I generated all of the ustacks code for each individual forward sample file. You can alter the ustacks code within this script (line 6). This file is designed to keep me from having to copy and paste all of my new sample IDs into the same ustacks code.\n",
    "<br>\n",
    "Default Parameters: **-m 5 -M 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python ustacks_replaceIDs.py L1_filename_conversion.txt #generates new_ustacks_shell\n",
    "!mv new_ustacks_shell ../L1F_ustacks_shell #rename script to make it more informative, move up one directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** I also opened the `L1F_ustacks_shell` file and added: \"#!/bin/bash\" at the top so that it runs as a shell script.\n",
    "\n",
    "**(3)** I ran the ustacks shell script from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ../\n",
    "!chmod +x L1F_ustacks_shell\n",
    "!./L1F_ustacks_shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output 4 .tsv.gz files per individual: (1) alleles, (2) models, (3) SNPs, (4) tags\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "I then moved the `L1F_ustacks_shell` into the `scripts/Lane1` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv L1F_ustacks_shell scripts/Lane1/L1F_ustacks_shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10/12/2016\n",
    "\n",
    "### Running cstacks\n",
    "\n",
    "**(1)** I do not have a reference genome, so to create my catalog I used 3 individuals from each of the populations represented in the data set (total of 12 individuals; 4 populations = Namhae, Geoje, Pohang, and Yellow Sea Block). I only have 3 individuals from the Yellow Sea population, so they will all be used for the catalog. For the remaining populations, I want to pick the individuals with the most data available; aka the largest data file sizes from the process_radtags output (in the SamplesT142 folder). The best way to find out this information is to determine how many lines are in each gzipped file. This can be accomplished using a pipeline on the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting in the root directory, DataAnalysis (github = mf-fish546-2016)\n",
    "cd samplesT142\n",
    "zcat [sampleID].1.fa.gz | wc -l >> ../new_WordCounts.txt   #using >> appends, rather than overwrites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *(a)* Rather than typing this for all individuals: I created a code that will generate (1) all of the word count code, and (2) a list of the gzfasta files names by using my original document, \"L1_mv_barcodesTOsample.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python cstacks_generateLineCountCode.py  L1_mv_barcodesTOsample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zcat PO010715_06.1.fa.gz | wc -l >> ../new_LineCounts.txt\r\n",
      "zcat PO010715_27.1.fa.gz | wc -l >> ../new_LineCounts.txt\r\n",
      "zcat PO010715_28.1.fa.gz | wc -l >> ../new_LineCounts.txt\r\n",
      "zcat PO010715_29.1.fa.gz | wc -l >> ../new_LineCounts.txt\r\n",
      "zcat GE011215_08.1.fa.gz | wc -l >> ../new_LineCounts.txt\r\n",
      "zcat GE011215_09.1.fa.gz | wc -l >> ../new_LineCounts.txt\r\n",
      "zcat GE011215_14.1.fa.gz | wc -l >> ../new_LineCounts.txt\r\n",
      "zcat GE011215_15.1.fa.gz | wc -l >> ../new_LineCounts.txt\r\n",
      "zcat NA021015_16.1.fa.gz | wc -l >> ../new_LineCounts.txt\r\n",
      "zcat NA021015_21.1.fa.gz | wc -l >> ../new_LineCounts.txt\r\n"
     ]
    }
   ],
   "source": [
    "!head new_cstacks_linecount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PO010715_06.1.fa.gz\r\n",
      "PO010715_27.1.fa.gz\r\n",
      "PO010715_28.1.fa.gz\r\n",
      "PO010715_29.1.fa.gz\r\n",
      "GE011215_08.1.fa.gz\r\n",
      "GE011215_09.1.fa.gz\r\n",
      "GE011215_14.1.fa.gz\r\n",
      "GE011215_15.1.fa.gz\r\n",
      "NA021015_16.1.fa.gz\r\n",
      "NA021015_21.1.fa.gz\r\n"
     ]
    }
   ],
   "source": [
    "!head new_cstacks_linecountSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/samplesT142\n"
     ]
    }
   ],
   "source": [
    "cd ../samplesT142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "#copy and paste the text from file \"new_cstacks_linecount\" into the command line\n",
    "#this produces a file \"new_LineCounts.txt\" in the root directory\n",
    "cd ../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv new_cstacks_linecount Lane1/Lane1_cstacks_linecount\n",
    "!mv new_cstacks_linecountSamples Lane1/Lane1_cstacks_linecountSamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *(b)* I then copied and pasted the gzfasta file names and the line counts side-by-side in excel: file named `Lane1_sortedFilesbySize` in the `scripts/` folder.\n",
    "<br> \n",
    "I sorted by population and # of lines. From this list, I copied the filenames of the 3 largest files back into a text document, \"samples_for_cstacks.txt\". NOTE: I labeled the batch number of that group at the top. ALL lines that are NOT the filenames of this particular batch should be hashed out (#) for the next code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\r\n",
      "#batch 1: 12 individuals (3 from each)\r\n",
      "YS121315_14.1.fa.gz\r\n",
      "YS121315_10.1.fa.gz\r\n",
      "YS121315_08.1.fa.gz\r\n",
      "GE011215_30.1.fa.gz\r\n",
      "GE011215_15.1.fa.gz\r\n",
      "GE011215_14.1.fa.gz\r\n",
      "NA021015_06.1.fa.gz\r\n",
      "NA021015_13.1.fa.gz\r\n"
     ]
    }
   ],
   "source": [
    "!head samples_for_cstacks.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** I used the  \"samples_for_cstacks.txt\" file and the code below to generate the code for cstacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python cstacks_generateCode2.py samples_for_cstacks.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** I **renamed the batch number,** then coped and pasted the generated code into the command line to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ../ #to root directory\n",
    "mkdir stacks #make folder \"stacks\" for output\n",
    "\n",
    "!cstacks -b 1 \\\n",
    "-s stacks/YS121315_14.1 -s stacks/YS121315_10.1 -s stacks/YS121315_08.1 -s stacks/GE011215_30.1 -s stacks/GE011215_15.1 -s stacks/GE011215_14.1 -s stacks/NA021015_06.1 -s stacks/NA021015_13.1 -s stacks/NA021015_16.1 -s stacks/PO010715_17.1 -s stacks/PO020515_10.1 -s stacks/PO020515_08.1 \\\n",
    "-o stacks -n 3 -p 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this output 3 files into the stacks folder: (1) batch_1.catalog.alleles, (2)  batch_1.catalog.tags, (3)  batch_1.catalog.snps\n",
    "I copied the standard output in terminal into this text file: cstacksOut_batch1\n",
    "\n",
    "From this output, it looks like every time a new locus is discovered in any sample it is added, even if the locus is only found in that sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#renamed batch numbers to be more informative (total of 12 individuals in catalog)\n",
    "cd stacks\n",
    "!mv batch_1.catalog.alleles batch_12.catalog.alleles\n",
    "!mv batch_1.catalog.tags batch_12.catalog.tags\n",
    "!mv batch_1.catalog.snps batch_12.catalog.snps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the number of individuals per pop to build a catalog: from Charlie's notes\n",
    "The number of loci retained much farther down the pipeline is the greatest when 10 individuals per population were chosen to build the catalog (tested n = 10, 25, 50, all). Not sure if using less than ten would further increase number of loci retained. This trend held constant across all error rates, when looking at corrected and uncorrected genotypes.\n",
    "\n",
    "So...\n",
    "\n",
    "I reran everything above with 10 individuals from GEO, POH, NAM, and the 3 individuals from YS. (batch = 33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ../\n",
    "\n",
    "cstacks -b 33 \\\n",
    "-s stacks/YS121315_14.1 -s stacks/YS121315_10.1 -s stacks/YS121315_08.1 \\\n",
    "-s stacks/GE011215_07.1 -s stacks/GE012315_06.1 -s stacks/GE012315_11.1 -s stacks/GE012315_03.1 -s stacks/GE011215_09.1 -s stacks/GE011215_16.1 -s stacks/GE011215_01.1 -s stacks/GE011215_30.1 -s stacks/GE011215_15.1 -s stacks/GE011215_14.1 \\\n",
    "-s stacks/NA021015_10.1 -s stacks/NA021015_17.1 -s stacks/NA021015_21.1 -s stacks/NA021015_02.1 -s stacks/NA021015_09.1 -s stacks/NA021015_22.1 -s stacks/NA021015_14.1 -s stacks/NA021015_06.1 -s stacks/NA021015_13.1 -s stacks/NA021015_16.1 \\\n",
    "-s stacks/PO020515_03.1 -s stacks/PO010715_10.1 -s stacks/PO020515_14.1 -s stacks/PO010715_11.1 -s stacks/PO020515_05.1 -s stacks/PO020515_17.1 -s stacks/PO020515_15.1 -s stacks/PO010715_17.1 -s stacks/PO020515_10.1 -s stacks/PO020515_08.1 \\\n",
    "-o stacks -n 3 -p 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this output 3 files into the stacks folder: (1) batch_33.catalog.alleles, (2)  batch_33.catalog.tags, (3)  batch_33.catalog.snps. I saved this output in \"cstacksOut_batch33\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**!! NOTE !!** \n",
    "\n",
    "I later re-ran this WITHOUT the Yellow Sea samples (batch_303), so this batch ended up being deleted from my files. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
