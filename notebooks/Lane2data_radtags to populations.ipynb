{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequencing Lane Two Data\n",
    "\n",
    "\n",
    "This notebook runs the Korean P. cod Lane 2 RADseq data through the stacks pipeline, from `process_radtags` through `populations`. The parameters used in this notebook were established using the Lane 1 data (see Testing Stacks Parameters I and II). \n",
    "This lane of sequencing includes important samples to be processed *before* continuing with the rest of lab work, in order to test the effectiveness of the protocols used on fully degraded DNA and 300ng DNA. \n",
    "\n",
    "This [Evernote nb](http://www.evernote.com/l/AoqJfZuhLQRLGIQ39UkIkKLxdGYsBSXFxt0/) contains the correct version of all the scripts used here, as well as FastQC and other visuals\n",
    "\n",
    "\n",
    "**Data info: **\n",
    "\n",
    "Illumina HiSeq 4000 SR 150bp\n",
    "\n",
    "Run 820, Sample 768\n",
    "\n",
    "This lane of sequencing contains 72 samples and 60 unique samples. This includes: \n",
    "- 12 individuals with fully degraded DNA\n",
    "- 12 individuals prepared with 500ng and with 300ng DNA (NEW 300ng protocol for testing)\n",
    "\n",
    "\n",
    "**Programs:** FastQC, stacks v. 1.42 \n",
    "<br><br><br><br>\n",
    "\n",
    "### 11/14/2016\n",
    "\n",
    "**STEP ONE: DOWNLOAD DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/raw_data\n"
     ]
    }
   ],
   "source": [
    "cd raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[34;42mL1_PE150\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir L2_SR150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[34;42mL1_PE150\u001b[0m/  \u001b[34;42mL2_SR150\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/raw_data/L2_SR150\n"
     ]
    }
   ],
   "source": [
    "cd L2_SR150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#not enough space on C: to download; used \n",
    "wget -r --no-parent --reject \"index.html*\" http://gc3fstorage.uoregon.edu/HGGJ2BBXX/768/  \n",
    "#to download directly into folder on D: instead of C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32m768_768_S99_L002_R1_001.fastq.gz\u001b[0m*  \u001b[34;42mFlowcell\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP TWO: CHECK OUT FASTQC **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/fastqc_v0.11.5/FastQC\n"
     ]
    }
   ],
   "source": [
    "cd ../../fastqc_v0.11.5/FastQC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "!./fastqc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links to output can be found in this [Evernote nb](http://www.evernote.com/l/AoqJfZuhLQRLGIQ39UkIkKLxdGYsBSXFxt0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP THREE: `process_radtags`**\n",
    "\n",
    "I want to run process_radtags and THEN run ustacks, rather than using a shell script for both, because I want to check out several samples post-process_radtags to see if the trimming actually got rid of enough poor quality bases at the end of the read. \n",
    "\n",
    "I also won't be running all samples through ustacks at first, since I haven't decided on the parameters at the moment. \n",
    "\n",
    "All of the process_radtags sample will be in the `L2samplesT142/` folder. I won't be renaming the lane 1 `samplesT142/` folder to include \"L1\" since all of my notes refer to it as just `samplesT142` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir L2samplesT142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mcstacks_b303\u001b[0m*                        \u001b[34;42mL2samplesT142\u001b[0m/              \u001b[34;42msamplesT142\u001b[0m/\r\n",
      "\u001b[34;42mDiagrams\u001b[0m/                            \u001b[34;42mmf-fish546-2016\u001b[0m/            \u001b[34;42msamplesT146\u001b[0m/\r\n",
      "\u001b[34;42mfastqc_v0.11.5\u001b[0m/                      \u001b[01;32mprocess_radtags_nobarcode\u001b[0m*  \u001b[34;42mscripts\u001b[0m/\r\n",
      "\u001b[01;32milluminaFilter_process_radtags.log\u001b[0m*  \u001b[34;42mraw_data\u001b[0m/                   \u001b[34;42mUCstacksL1\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I made a barcode + sampleID text file for process_radtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can create a shell script for process_radtags --> ustacks, and run the `process_radtags` part of it in the command line. Note that instead of changing the file names from barcode --> sample ID myself, I can simply specify a barcode file to `process_radtags` in the format: \n",
    "<br>\n",
    "<br>\n",
    "barcode \\t sampleID\n",
    "<br>\n",
    "<br>\n",
    "So I made a barcode_sampleID text file without the \"mv\" that I used in Lane 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% more barcodes_run01_lane02\r\n",
      "CAAAAA\tSO022216_01\r\n",
      "GACGAC\tMU011816_01\r\n",
      "TTTGTC\tMU012816_05\r\n",
      "TGCTTA\tMU012816_06\r\n",
      "CCCGGT\tMU012816_07\r\n",
      "GCGACC\tMU012816_08\r\n",
      "CTTATG\tMU012816_09\r\n",
      "AGCGCA\tMU012816_10\r\n",
      "TCGCCA\tMU032315_01\r\n"
     ]
    }
   ],
   "source": [
    "!head barcodesL2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python radtags_ustacks_genShellSR.py barcodesL2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** trim to 142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!process_radtags -p /mnt/hgfs/Shared\\ Drive\\ D/Pacific\\ cod/DataAnalysis/raw_data/L2_SR150/ \\\n",
    "-i gzfastq -y gzfastq \\\n",
    "-o /mnt/hgfs/Shared\\ Drive\\ D/Pacific\\ cod/DataAnalysis/L2samplesT142 \\\n",
    "-b /mnt/hgfs/Shared\\ Drive\\ D/Pacific\\ cod/DataAnalysis/scripts/barcodesL2.txt \\\n",
    "-e sbfI -E phred33 -r -c -q -t 142"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing single-end data.\n",
    "\n",
    "Using Phred+33 encoding for quality scores.\n",
    "\n",
    "Reads will be truncated to 142bp\n",
    "\n",
    "Found 1 input file(s).\n",
    "\n",
    "Searching for single-end, inlined barcodes.\n",
    "\n",
    "Loaded 72 barcodes (6bp).\n",
    "\n",
    "Will attempt to recover barcodes with at most 1 mismatches.\n",
    "\n",
    "Processing file 1 of 1 [768_768_S99_L002_R1_001.fastq.gz]\n",
    "  377851294 total reads; -50386443 ambiguous barcodes; -65025476 ambiguous RAD-Tags; +41406050 recovered; -2840924 low quality reads; 259598451 retained reads.\n",
    "  \n",
    "Closing files, flushing buffers...\n",
    "\n",
    "Outputing details to log: '/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/L2samplesT142/process_radtags.log'\n",
    "<br>\n",
    "<br>\n",
    "**377851294** total sequences;\n",
    "\n",
    "  **50386443** ambiguous barcode drops;\n",
    "  \n",
    " ** 2840924** low quality read drops;\n",
    "  \n",
    "  **65025476** ambiguous RAD-Tag drops;\n",
    "  \n",
    "**259598451** retained reads.\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "Retained 68.7% reads\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking FASTQC output per sample**\n",
    "\n",
    "I opened at least one sample from each of the populations + breeding seasons in FastQC to check out the per base sequence quality toward the end of the sample. \n",
    "\n",
    "See [Evernote](http://www.evernote.com/l/AoqJfZuhLQRLGIQ39UkIkKLxdGYsBSXFxt0/)\n",
    "<br>\n",
    "<br>\n",
    "Summary: most of the FastQC output looks fine, with PBSQ not in the red at all, and dropping into the yellow between 115 - 125 bp in the NON-DEGRADED sequences. In several DEGRADED sequences, PBSQ drops into the red at 105-130. \n",
    "<br>\n",
    "MU011816_01: 130\n",
    "\n",
    "MU012816_05: 110\n",
    "\n",
    "MU012816_10: 104\n",
    "\n",
    "MU032315_01: 140\n",
    "\n",
    "MU032315_02: 130\n",
    "\n",
    "MU033015_03: 94\n",
    "\n",
    "\n",
    "These samples also have the smallest file sizes: \n",
    "<br>\n",
    "MU033015_03: 23.1\n",
    "\n",
    "MU032315_02: 28.9\n",
    "\n",
    "MU012816_10: 36.2\n",
    "\n",
    "(MU012816_07: 37.6)\n",
    "\n",
    "(MU033015_02: 38.8)\n",
    "\n",
    "(MU012816_09: 39.3)\n",
    "\n",
    "MU012816_05: 58.6\n",
    "\n",
    "MU032315_01: 68.8\n",
    "\n",
    "(SO022216_01: 77.9)\n",
    "\n",
    "MU011816_01: 92.3\n",
    "\n",
    "(MU012816_06: 100.7)\n",
    "\n",
    "(MU012816_08: 144.3)\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**(2)** Trim to 110\n",
    "\n",
    "I'm trimming all of the samples much smaller so that I can selectively look at the fully degraded samples, and see if more reads are retained (larger file size) when I get rid of the end of the sequence that might be causing those reads to be filtered out. If I do end up using these trimmed samples later, I would only use the ones in Mukho/Sokcho. This isolates the smaller sequences to a single population, and when making comparisons with this population I could selectively focus on SNPs at positions below 110 bp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir L2samplesT110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!process_radtags -p /mnt/hgfs/Shared\\ Drive\\ D/Pacific\\ cod/DataAnalysis/raw_data/L2_SR150/ \\\n",
    "-i gzfastq -y gzfastq \\\n",
    "-o /mnt/hgfs/Shared\\ Drive\\ D/Pacific\\ cod/DataAnalysis/L2samplesT110 \\\n",
    "-b /mnt/hgfs/Shared\\ Drive\\ D/Pacific\\ cod/DataAnalysis/scripts/barcodesL2.txt \\\n",
    "-e sbfI -E phred33 -r -c -q -t 110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing single-end data.\n",
    "\n",
    "Using Phred+33 encoding for quality scores.\n",
    "\n",
    "Reads will be truncated to 110bp\n",
    "\n",
    "Found 1 input file(s).\n",
    "\n",
    "Searching for single-end, inlined barcodes.\n",
    "\n",
    "Loaded 72 barcodes (6bp).\n",
    "\n",
    "Will attempt to recover barcodes with at most 1 mismatches.\n",
    "\n",
    "Processing file 1 of 1 [768_768_S99_L002_R1_001.fastq.gz]\n",
    "\n",
    "  377851294 total reads; -50386443 ambiguous barcodes; -65025476 ambiguous RAD-Tags; +41406050 recovered; -2422281 low quality reads; 260017094 retained reads.\n",
    "  \n",
    "Closing files, flushing buffers...\n",
    "\n",
    "Outputing details to log: '/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/L2samplesT110/process_radtags.log'\n",
    "<br>\n",
    "<br>\n",
    "377851294 total sequences;\n",
    "\n",
    "  50386443 ambiguous barcode drops;\n",
    "  \n",
    "  2422281 low quality read drops;\n",
    "  \n",
    "  65025476 ambiguous RAD-Tag drops;\n",
    "  \n",
    "260017094 retained reads.\n",
    "\n",
    "________________________________________________________________________________________\n",
    "\n",
    "Retained 68.8% reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECKING PROTOCOL RESULTS (post `process_radtags`)\n",
    "\n",
    "\n",
    "To compare the effectiveness of the 300ng protocol, I want to compare the number of reads retained for each of the 12 individuals that were run with 500ng and with 300ng of DNA. In order to do this, I'll run a shell script that will use `awk` to extract the number of sequences from each file. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "**(1)** Made a shell script to count the sequences in each FastQ file, and output to FastQsequenceCounts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "\r\n",
      "\r\n",
      "cd ../L2samplesT142\r\n",
      "\r\n",
      "zcat ../L2samplesT142/GEO020414_8.fq | awk '((NR-2)%4==0){read=$1;total++;count[read]++}END{for(read in count){if(!max||count[read]>max) {max=count[read];maxRead=read};if(count[read]==1){unique++}};print total,unique,unique*100/total,maxRead,count[maxRead],count[maxRead]*100/total}' >> FastQsequenceCounts.txt\r\n",
      "\r\n",
      "zcat ../L2samplesT142/GEO020414_9.fq | awk '((NR-2)%4==0){read=$1;total++;count[read]++}END{for(read in count){if(!max||count[read]>max) {max=count[read];maxRead=read};if(count[read]==1){unique++}};print total,unique,unique*100/total,maxRead,count[maxRead],count[maxRead]*100/total}' >> FastQsequenceCounts.txt\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head countFASTQ_seqs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zcat ../L2samplesT142/GEO020414_23_300.fq.gz | awk '((NR-2)%4==0){read=$1;total++;count[read]++}END{for(read in count){if(!max||count[read]>max) {max=count[read];maxRead=read};if(count[read]==1){unique++}};print total,unique,unique*100/total,maxRead,count[maxRead],count[maxRead]*100/total}' >> FastQsequenceCounts_300.txt\r\n",
      "\r\n",
      "zcat ../L2samplesT142/GEO020414_24_300.fq.gz | awk '((NR-2)%4==0){read=$1;total++;count[read]++}END{for(read in count){if(!max||count[read]>max) {max=count[read];maxRead=read};if(count[read]==1){unique++}};print total,unique,unique*100/total,maxRead,count[maxRead],count[maxRead]*100/total}' >> FastQsequenceCounts_300.txt\r\n",
      "\r\n",
      "zcat ../L2samplesT142/GEO020414_25_300.fq.gz | awk '((NR-2)%4==0){read=$1;total++;count[read]++}END{for(read in count){if(!max||count[read]>max) {max=count[read];maxRead=read};if(count[read]==1){unique++}};print total,unique,unique*100/total,maxRead,count[maxRead],count[maxRead]*100/total}' >> FastQsequenceCounts_300.txt\r\n",
      "\r\n",
      "zcat ../L2samplesT142/YS121315_12_300.fq.gz | awk '((NR-2)%4==0){read=$1;total++;count[read]++}END{for(read in count){if(!max||count[read]>max) {max=count[read];maxRead=read};if(count[read]==1){unique++}};print total,unique,unique*100/total,maxRead,count[maxRead],count[maxRead]*100/total}' >> FastQsequenceCounts_300.txt\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!tail countFASTQ_seqs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod +x countFASTQ_seqs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "!./countFASTQ_seqs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd also like to take a look at the degraded DNA samples, to see how many sequences were retained. I used the same format as above, simply changing the sample names and changing the output to `>>FastQseqCounts_degraded.txt`\n",
    "\n",
    "I'm a little worried because the files for the degraded samples seem fairly small, especially considering that they are FastQ files, and so should be larger than the Fasta file from Lane 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "\r\n",
      "\r\n",
      "cd ../L2samplesT142\r\n",
      "\r\n",
      "zcat MU033015_03.fq.gz | awk '((NR-2)%4==0){read=$1;total++;count[read]++}END{for(read in count){if(!max||count[read]>max) {max=count[read];maxRead=read};if(count[read]==1){unique++}};print total,unique,unique*100/total,maxRead,count[maxRead],count[maxRead]*100/total}' >> FastQseqCounts_deg.txt\r\n",
      "\r\n",
      "zcat MU032315_02.fq.gz | awk '((NR-2)%4==0){read=$1;total++;count[read]++}END{for(read in count){if(!max||count[read]>max) {max=count[read];maxRead=read};if(count[read]==1){unique++}};print total,unique,unique*100/total,maxRead,count[maxRead],count[maxRead]*100/total}' >> FastQseqCounts_deg.txt\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head countFASTQ_seqs_deg.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x countFASTQ_seqs_deg.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!./countFASTQ_seqs_deg.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outcomes**\n",
    "<br>\n",
    "For graphs and scripts, see this [Evernote notebook](http://www.evernote.com/l/AoqJfZuhLQRLGIQ39UkIkKLxdGYsBSXFxt0/). \n",
    "<br>\n",
    "<br>\n",
    "(1) 300ng and 500ng : significant different between the total number of sequences and the number of unique sequences using a paired t-test. The 500ng set had MORE total and unique sequences than the 300ng set. However, graphically, this doesn't look too bad (excluding GEO020414_13, which for some reason had very low # of reads. Will have to use 300ng for this one).\n",
    "On average, a 21% increase in total # of reads from 300ng to 500ng (min = 9.3%, max = 43.4%)\n",
    "On average, a 20% increase in total # unique reads from 300ng to 500ng (min = 7%, max = 40%)\n",
    "<br>\n",
    "\n",
    "(2) Degraded DNA, trimmed to 142 v. 110: Highly significant difference between total # and # of unique sequences, % of sequences unique (????) but on a graph you can barely tell the difference; much less obvious than 300 v. 500 comparison.\n",
    "On average, a 3% increase in # of reads from trimming to 142 to 110 bp. Will stick with the 142\n",
    "<br>\n",
    "\n",
    "(3) Degraded DNA v. Good DNA (500ng) : Ouch. Highly significant differences in all four categories. The average difference is 3.31 * 10^6 (good DNA WAY better).\n",
    "On average, good quality DNA samples looked at here have 4.59 x the number of reads as 500ng of degraded DNA.\n",
    "<br>\n",
    "____________________________________________________\n",
    "______________________________________________________\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP FOUR: RUN USTACKS **\n",
    "<br>\n",
    "<br>\n",
    "I'll run ustacks on the T142 samples, with the parameters that I THINK will end up being my defaults: \n",
    "\n",
    "**-m** 10\n",
    "**-M** 3\n",
    "\n",
    "These are identical to the runs on the Lane 1 data in the `stacks_m10` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to create a shell script that runs ustacks AND \n",
    "# finds seq counts for all fq.gz files, use to determine which samples used for cstacks\n",
    "!python radtags_ustacks_genShellSR.py barcodesL2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv new_radtags_ustacks_shell ../ustacks_shell_11-15.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "\r\n",
      "#ustacks\r\n",
      "cd /mnt/hgfs/Shared\\ Drive\\ D/Pacific\\ cod/DataAnalysis\r\n",
      "ustacks -t gzfastq -f L2samplesT142/SO022216_01.fq.gz -r -d -o stacks_m10 -i 001 -m 10 -M 3 -p 6\r\n",
      "ustacks -t gzfastq -f L2samplesT142/MU011816_01.fq.gz -r -d -o stacks_m10 -i 002 -m 10 -M 3 -p 6\r\n",
      "ustacks -t gzfastq -f L2samplesT142/MU012816_05.fq.gz -r -d -o stacks_m10 -i 003 -m 10 -M 3 -p 6\r\n",
      "ustacks -t gzfastq -f L2samplesT142/MU012816_06.fq.gz -r -d -o stacks_m10 -i 004 -m 10 -M 3 -p 6\r\n",
      "ustacks -t gzfastq -f L2samplesT142/MU012816_07.fq.gz -r -d -o stacks_m10 -i 005 -m 10 -M 3 -p 6\r\n",
      "ustacks -t gzfastq -f L2samplesT142/MU012816_08.fq.gz -r -d -o stacks_m10 -i 006 -m 10 -M 3 -p 6\r\n"
     ]
    }
   ],
   "source": [
    "# deleted process_radtags part\n",
    "!head ustacks_shell_11-15.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir stacksCombo_m10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x ustacks_shell_11-15.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!./ustacks_shell_11-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plan going forward...***\n",
    "\n",
    "Move all of the ustacks files from Lane 1 in the UCstacksL1/stacks_m10 folder into the new stacksCombo_m10/ folder. \n",
    "\n",
    "Run this set of samples all the way through the stacks process, following the [flowchart](https://github.com/mfisher5/mf-fish546-2016/blob/master/Diagrams/PopGen_Workflow.md) on github. EXTRA STEPS: Marine's filtering scripts on `populations` genepop output. \n",
    "\n",
    "\n",
    "Then, compare between samples of interest:\n",
    "(1) # loci retained post filtering\n",
    "(2) % missing genotypes\n",
    "(3) # individuals that had to be taken out because of missing genotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 11/17/2016\n",
    "\n",
    "**STEP FIVE: RUN CSTACKS**\n",
    "\n",
    "I ran the same script used to **Check protocol results: post-process_radtags** to count the number of sequences in the gzipped fastq files from ustacks, and then added these to a spreadsheet with the lane 1 sequence counts. I ordered from largest to smallest, and took the first ten individuals from each population. I ignored the BORYEONG 2007 samples. For populations with less than ten individuals, I used all individuals. \n",
    "\n",
    "MUK/SOK 2015/2016 (12)\n",
    "\n",
    "GEO 2014 (33)\n",
    "\n",
    "GEO 2015 (33)\n",
    "\n",
    "POH 2015 (28)\n",
    "\n",
    "NAM 2015 (16)\n",
    "\n",
    "YS 2016 (7)\n",
    "\n",
    "I moved all of the ustacks files into the stacksCombo_m10 folder, and ran cstacks + sstacks with the following shell script: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/mf-fish546-2016/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### batch 1 : 10 individuals per pop except for Yellow Sea, stacksCombo_m10\r\n",
      "YS121315_12.fq.gz\r\n",
      "YS121315_15.fq.gz\r\n",
      "YS121315_16.fq.gz\r\n",
      "YS121315_08.1.fa.gz\r\n",
      "YS121315_10.1.fa.gz\r\n",
      "YS121315_14.1.fa.gz\r\n",
      "YS121315_12_300.fq.gz\r\n",
      "GE011215_07.1.fa.gz\r\n",
      "GE012315_06.1.fa.gz\r\n"
     ]
    }
   ],
   "source": [
    "!head samples_for_cstacks_L2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### Generate Shell Script that will run cstacks and sstacks #######\r\n",
      "## MF 11/16/2016\r\n",
      "## Command Line Arguments: \r\n",
      "#ARG1 = samples_for_cstacks file\r\n",
      "#ARG2 = first lane barcode file\r\n",
      "#ARG3 = second lane barcode file\r\n",
      "\r\n",
      "##########################################################################\r\n",
      "\r\n",
      "\r\n",
      "import sys\r\n",
      "newfile = open(\"cstacks_sstacks_shell_11-16.sh\", \"w\")\r\n",
      "## cstacks ##\r\n",
      "catFile = open(sys.argv[1], \"r\")\t#open the file with your list of samples to use in cstacks\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 15 cstacks_sstacks_genShellSR.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to input different barcode files for each lane because lane 1 samples have an additional `.1` extension on them, so they were handled differently in the python script above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate shell script\n",
    "!python cstacks_sstacks_genShellSR.py samples_for_cstacks_L2.txt barcodes_L1.txt barcodes_L2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#move to DataAnalysis folder\n",
    "!mv cstacks_sstacks_shell_11-16.sh ../cstacks_sstacks_shell_11-16.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x cstacks_sstacks_shell_11-16.sh\n",
    "!./cstacks_sstacks_shell_11-16.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, several of the `.matches` files generated by sstacks were 0 bytes. so I ran the following script to rerun these samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "sstacks -b 1 -c stacksCombo_m10/batch_1 -s stacksCombo_m10/GE012315_11.1 -s stacksCombo_m10/GEO020414_11 -s stacksCombo_m10/GEO020414_15_300 -s stacksCombo_m10/GEO020414_17_300 -s stacksCombo_m10/GEO020414_30 -o stacksCombo_m10 -p 6 2>> stacksCombo_m10/sstacks_out_b1.2\r\n"
     ]
    }
   ],
   "source": [
    "!head sstacks_11-16.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was also worried about previous samples run through sstacks; maybe they had missing data, but it wasn't obvious because they had a nonzero file size. so I reran my full sstacks script from WITHIN the stacksCombo_m10 folder; still, several files had 0 bytes, so I had to rerun them hand from the command line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/stacksCombo_m10\n"
     ]
    }
   ],
   "source": [
    "cd stacksCombo_m10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "\r\n",
      "sstacks -b 1 -c ./batch_1 -s ./PO010715_06.1 -s ./PO010715_27.1 -s ./PO010715_28.1 -s ./PO010715_29.1 -s ./GE011215_08.1 -s ./GE011215_09.1 -s ./GE011215_14.1 -s ./GE011215_15.1 -s ./NA021015_16.1 -s ./NA021015_21.1 -s ./GE011215_10.1 -s ./GE012315_01.1 -s ./PO020515_05.1 -s ./PO020515_09.1 -s ./PO020515_10.1 -s ./GE011215_07.1 -s ./GE011215_16.1 -s ./GE011215_29.1 -s ./NA021015_02.1 -s ./NA021015_03.1 -s ./NA021015_08.1 -s ./NA021015_13.1 -s ./GE012315_03.1 -s ./GE012315_22.1 -s ./GE012315_04.1 -s ./GE012315_05.1 -s ./GE012315_06.1 -s ./PO010715_19.1 -s ./PO031715_20.1 -s ./NA021015_10.1 -s ./GE011215_20.1 -s ./PO020515_03.1 -s ./PO020515_08.1 -s ./NA021015_17.1 -s ./NA021015_22.1 -s ./PO010715_11.1 -s ./PO020515_16.1 -s ./GE011215_21.1 -s ./GE011215_30.1 -s ./NA021015_14.1 -s ./NA021015_06.1 -s ./NA021015_09.1 -s ./PO020515_17.1 -s ./PO010715_17.1 -s ./PO020515_15.1 -s ./PO010715_10.1 -s ./GE011215_01.1 -s ./GE011215_24.1 -s ./PO031715_13.1 -s ./PO010715_08.1 -s ./PO020515_14.1 -s ./GE012315_08.1 -s ./GE012315_09.1 -s ./GE012315_10.1 -s ./GE012315_11.1 -s ./GE012315_17.1 -s ./GE012315_20.1 -s ./YS121315_08.1 -s ./YS121315_10.1 -s ./YS121315_14.1 -s ./SO022216_01 -s ./MU011816_01 -s ./MU012816_05 -s ./MU012816_06 -s ./MU012816_07 -s ./MU012816_08 -s ./MU012816_09 -s ./MU012816_10 -s ./MU032315_01 -s ./MU032315_02 -s ./MU033015_02 -s ./MU033015_03 -s ./GEO020414_8 -s ./GEO020414_9 -s ./GEO020414_11 -s ./GEO020414_13 -s ./GEO020414_14 -s ./GEO020414_15 -s ./GEO020414_16 -s ./GEO020414_17 -s ./GEO020414_23 -s ./GEO020414_24 -s ./GEO020414_25 -s ./YS121315_12 -s ./GEO020414_3 -s ./GEO020414_4 -s ./GEO020414_5 -s ./GEO020414_6 -s ./GEO020414_27 -s ./GEO020414_29 -s ./GEO012315_02 -s ./GEO012315_12 -s ./GEO012315_18 -s ./GEO012315_21 -s ./GEO020414_2 -s ./GEO020414_26 -s ./PO020515_06 -s ./PO031715_23 -s ./GE011215_18 -s ./GE011215_22 -s ./BOR07_01 -s ./BOR07_03 -s ./BOR07_09 -s ./GEO020414_7 -s ./GEO020414_10 -s ./GEO020414_30 -s ./YS121315_15 -s ./YS121315_16 -s ./PO031715_03 -s ./NA021015_23 -s ./PO010715_04 -s ./PO020515_01 -s ./PO031715_04 -s ./NA021015_25 -s ./NA021015_26 -s ./NA021015_30 -s ./PO031715_24 -s ./GE011215_19 -s ./PO010715_12 -s ./GE011215_28 -s ./GEO020414_8_300 -s ./GEO020414_9_300 -s ./GEO020414_11_300 -s ./GEO020414_13_300 -s ./GEO020414_14_300 -s ./GEO020414_15_300 -s ./GEO020414_16_300 -s ./GEO020414_17_300 -s ./GEO020414_23_300 -s ./GEO020414_24_300 -s ./GEO020414_25_300 -s ./YS121315_12_300 -o ./ -p 6 2>> sstacks_out_b1.3 \r\n"
     ]
    }
   ],
   "source": [
    "!head sstacks_shell_11-17.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See files `sstacks_out_b1`, `sstacks_out_b1.2` and `sstacks_out_b1.3` to look at standard error output. not sure why it won't work, no errors being thrown!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP SIX: RUN POPULATIONS**\n",
    "\n",
    "In order to filter the cstacks catalog of loci, I need to make a `.fasta` file of all of the unique loci + their corresponding sequences. I can get the sequences from the `.catalog.tags` file out of cstacks, but I have to run `populations` and get a genepop file to find the unique loci. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/stacksCombo_m10'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fst kernel smoothing: off\n",
      "Bootstrap resampling: off\n",
      "Percent samples limit per population: 0.5\n",
      "Locus Population limit: 2\n",
      "Minimum stack depth: 5\n",
      "Log liklihood filtering: off; threshold: 0\n",
      "Minor allele frequency cutoff: 0\n",
      "Maximum observed heterozygosity cutoff: 1\n",
      "Applying Fst correction: none.\n",
      "Parsing population map...\n",
      "The population map contained 129 samples, 6 population(s), 1 group(s).\n",
      "Error: Unable to locate any file in input directory 'stacksCombo_m10/'.\n"
     ]
    }
   ],
   "source": [
    "!populations -b 1 \\\n",
    "-P stacksCombo_m10 \\\n",
    "-M scripts/PopMap_stacksCombo_m10.txt \\\n",
    "-t 36 \\\n",
    "-r 0.50 \\\n",
    "-p 2 \\\n",
    "-m 5 \\\n",
    "--genepop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WTF???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis/stacksCombo_m10\n"
     ]
    }
   ],
   "source": [
    "cd stacksCombo_m10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cstacks version 1.42; catalog generated on 2016-11-16 21:25:45\r\n",
      "0\t1\t1\tAG\t0\t0\r\n",
      "0\t1\t1\tAT\t0\t0\r\n",
      "0\t1\t1\tTG\t0\t0\r\n",
      "0\t1\t1\tTT\t0\t0\r\n",
      "0\t1\t3\tAGC\t0\t0\r\n",
      "0\t1\t3\tCCC\t0\t0\r\n",
      "0\t1\t3\tCCG\t0\t0\r\n",
      "0\t1\t3\tCGC\t0\t0\r\n",
      "0\t1\t5\tA\t0\t0\r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!zcat batch_1.catalog.alleles.tsv.gz | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the files from cstacks or sstacks are corrupted in some way. Went back and reran everything. STILL outputting three files with no data; this time they were MUKHO samples (whereas last time they were GEOJE2014 samples). \n",
    "\n",
    "Tried running the following shell, which runs each sample individually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Shared Drive D/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "sstacks -b 1 -c stacksCombo_m10/batch_1 -s stacksCombo_m10/SO022216_01 -o stacksCombo_m10 -p 6\r\n",
      "sstacks -b 1 -c stacksCombo_m10/batch_1 -s stacksCombo_m10/MU011816_01 -o stacksCombo_m10 -p 6\r\n",
      "sstacks -b 1 -c stacksCombo_m10/batch_1 -s stacksCombo_m10/MU012816_05 -o stacksCombo_m10 -p 6\r\n",
      "sstacks -b 1 -c stacksCombo_m10/batch_1 -s stacksCombo_m10/MU012816_06 -o stacksCombo_m10 -p 6\r\n",
      "sstacks -b 1 -c stacksCombo_m10/batch_1 -s stacksCombo_m10/MU012816_07 -o stacksCombo_m10 -p 6\r\n",
      "sstacks -b 1 -c stacksCombo_m10/batch_1 -s stacksCombo_m10/MU012816_08 -o stacksCombo_m10 -p 6\r\n",
      "sstacks -b 1 -c stacksCombo_m10/batch_1 -s stacksCombo_m10/MU012816_09 -o stacksCombo_m10 -p 6\r\n",
      "sstacks -b 1 -c stacksCombo_m10/batch_1 -s stacksCombo_m10/MU012816_10 -o stacksCombo_m10 -p 6\r\n",
      "sstacks -b 1 -c stacksCombo_m10/batch_1 -s stacksCombo_m10/MU032315_01 -o stacksCombo_m10 -p 6\r\n"
     ]
    }
   ],
   "source": [
    "!head sstacks_byline_shell_11-17.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!populations -b 1 \\\n",
    "-P stacksCombo_m10 \\\n",
    "-M scripts/PopMap_stacksCombo_m10.txt \\\n",
    "-t 36 \\\n",
    "-r 0.50 \\\n",
    "-p 2 \\\n",
    "-m 5 \\\n",
    "--genepop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to generate the bowtie fasta file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis/stacksCombo_m10\n"
     ]
    }
   ],
   "source": [
    "cd stacksCombo_m10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!gzip -d batch_1.catalog.tags.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python genBOWTIEfasta.py ../stacksCombo_m10/batch_1loci.txt ../stacksCombo_m10/batch_1.catalog.tags.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">3\r\n",
      "TGCAGGACGACCCGCTGGGGCGTGGTGCTCTGCCAAATGGGTGGCAGTGAAGGTCAGGGGTCTCAGGGGACCAGACCCGACCGGCAGAAACTGGCTCAGGGGATGTGGAATGTCACCTCACTGTGGGGAAAGGAGCTTTAGC\r\n",
      ">5\r\n",
      "TGCAGGGATATTAAATACAGGCACAGGGAACATGGCAGGGTGGAAGAGGATTGGCCTGCGCTGGAACTTTTTATCATAAATTGGAGTCTGCGGAGATAGACTGCCGACTTGAGTGTGCGGTTTGCTCCAACAGCAAAGTGTT\r\n",
      ">14\r\n",
      "TGCAGGTACACACGCTCAAGTCACGTTGAGGCGTGTACTGTATGTTAAGTACATGTTAAGTAATGGTTAAGTATCCTTCCTCAGACTGAGGAAGGGTTATTCACACTACACGTGTGAATAATTATTTACACAACACGTTTTA\r\n",
      ">16\r\n",
      "TGCAGGGGCCGGGGACGGGGGTGGTGTGTATGCTCGGACATGTTTGTCGAGAGTTTAGGGGTAGTCTGTCCTGCTTCATGGTGACAGAGATGGCAATGGGAGTGAGTGAGTGCGCTGAGTCGCTGTGTAGTCACAGAGTGCT\r\n",
      ">17\r\n",
      "TGCAGGTTCAGGATGATCTTGTCCGGCCCAAAGCTGTTACTGGCCACGCAGGTGTAGTTCCCAGAATCCTCGGCTTTCACCGTGCGTATGACCAGGCTGCCGTTGCCGTGGACGCTGCGTCGGCTGTCAATCACAACAGGCG\r\n"
     ]
    }
   ],
   "source": [
    "!head seqsforBOWTIE.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis/stacksCombo_m10\n"
     ]
    }
   ],
   "source": [
    "cd stacksCombo_m10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP EIGHT: FILTER WITH BOWTIE AND BLAST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir BOWTIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv ../scripts/seqsforBOWTIE.fa BOWTIE/seqsforBOWTIE.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis/stacksCombo_m10/BOWTIE\n"
     ]
    }
   ],
   "source": [
    "cd BOWTIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14600\r\n"
     ]
    }
   ],
   "source": [
    "!grep \">\" seqsforBOWTIE.fa | wc -l #number of unique loci that were retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[34;42mbowtie-1.1.2\u001b[0m/  \u001b[01;32mseqsforBOWTIE.fa\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      "  Output files: \"batch_1.*.ebwt\"\n",
      "  Line rate: 6 (line is 64 bytes)\n",
      "  Lines per side: 1 (side is 64 bytes)\n",
      "  Offset rate: 5 (one in 32)\n",
      "  FTable chars: 10\n",
      "  Strings: unpacked\n",
      "  Max bucket size: default\n",
      "  Max bucket size, sqrt multiplier: default\n",
      "  Max bucket size, len divisor: 4\n",
      "  Difference-cover sample period: 1024\n",
      "  Endianness: little\n",
      "  Actual local endianness: little\n",
      "  Sanity checking: disabled\n",
      "  Assertions: disabled\n",
      "  Random seed: 0\n",
      "  Sizeofs: void*:8, int:4, long:8, size_t:8\n",
      "Input files DNA, FASTA:\n",
      "  seqsforBOWTIE.fa\n",
      "Reading reference sizes\n",
      "  Time reading reference sizes: 00:00:00\n",
      "Calculating joined length\n",
      "Writing header\n",
      "Reserving space for joined string\n",
      "Joining reference sequences\n",
      "  Time to join reference sequences: 00:00:00\n",
      "bmax according to bmaxDivN setting: 518300\n",
      "Using parameters --bmax 388725 --dcv 1024\n",
      "  Doing ahead-of-time memory usage test\n",
      "  Passed!  Constructing with these parameters: --bmax 388725 --dcv 1024\n",
      "Constructing suffix-array element generator\n",
      "Building DifferenceCoverSample\n",
      "  Building sPrime\n",
      "  Building sPrimeOrder\n",
      "  V-Sorting samples\n",
      "  V-Sorting samples time: 00:00:00\n",
      "  Allocating rank array\n",
      "  Ranking v-sort output\n",
      "  Ranking v-sort output time: 00:00:00\n",
      "  Invoking Larsson-Sadakane on ranks\n",
      "  Invoking Larsson-Sadakane on ranks time: 00:00:00\n",
      "  Sanity-checking and returning\n",
      "Building samples\n",
      "Reserving space for 12 sample suffixes\n",
      "Generating random suffixes\n",
      "QSorting 12 sample offsets, eliminating duplicates\n",
      "QSorting sample offsets, eliminating duplicates time: 00:00:00\n",
      "Multikey QSorting 12 samples\n",
      "  (Using difference cover)\n",
      "  Multikey QSorting samples time: 00:00:00\n",
      "Calculating bucket sizes\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Split 1, merged 7; iterating...\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Avg bucket size: 296171 (target: 388724)\n",
      "Converting suffix-array elements to index image\n",
      "Allocating ftab, absorbFtab\n",
      "Entering Ebwt loop\n",
      "Getting block 1 of 7\n",
      "  Reserving size (388725) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 365484\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 365485\n",
      "Getting block 2 of 7\n",
      "  Reserving size (388725) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 167082\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 167083\n",
      "Getting block 3 of 7\n",
      "  Reserving size (388725) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 321201\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 321202\n",
      "Getting block 4 of 7\n",
      "  Reserving size (388725) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 329901\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 329902\n",
      "Getting block 5 of 7\n",
      "  Reserving size (388725) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 359579\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 359580\n",
      "Getting block 6 of 7\n",
      "  Reserving size (388725) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 340910\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:01\n",
      "Returning block of 340911\n",
      "Getting block 7 of 7\n",
      "  Reserving size (388725) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 189037\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 189038\n",
      "Exited Ebwt loop\n",
      "fchr[A]: 0\n",
      "fchr[C]: 502716\n",
      "fchr[G]: 1031282\n",
      "fchr[T]: 1599992\n",
      "fchr[$]: 2073200\n",
      "Exiting Ebwt::buildToDisk()\n",
      "Returning from initFromVector\n",
      "Wrote 5104852 bytes to primary EBWT file: batch_1.1.ebwt\n",
      "Wrote 259156 bytes to secondary EBWT file: batch_1.2.ebwt\n",
      "Re-opening _in1 and _in2 as input streams\n",
      "Returning from Ebwt constructor\n",
      "Headers:\n",
      "    len: 2073200\n",
      "    bwtLen: 2073201\n",
      "    sz: 518300\n",
      "    bwtSz: 518301\n",
      "    lineRate: 6\n",
      "    linesPerSide: 1\n",
      "    offRate: 5\n",
      "    offMask: 0xffffffe0\n",
      "    isaRate: -1\n",
      "    isaMask: 0xffffffff\n",
      "    ftabChars: 10\n",
      "    eftabLen: 20\n",
      "    eftabSz: 80\n",
      "    ftabLen: 1048577\n",
      "    ftabSz: 4194308\n",
      "    offsLen: 64788\n",
      "    offsSz: 259152\n",
      "    isaLen: 0\n",
      "    isaSz: 0\n",
      "    lineSz: 64\n",
      "    sideSz: 64\n",
      "    sideBwtSz: 56\n",
      "    sideBwtLen: 224\n",
      "    numSidePairs: 4628\n",
      "    numSides: 9256\n",
      "    numLines: 9256\n",
      "    ebwtTotLen: 592384\n",
      "    ebwtTotSz: 592384\n",
      "    reverse: 0\n",
      "Total time for call to driver() for forward index: 00:00:02\n",
      "Reading reference sizes\n",
      "  Time reading reference sizes: 00:00:00\n",
      "Calculating joined length\n",
      "Writing header\n",
      "Reserving space for joined string\n",
      "Joining reference sequences\n",
      "  Time to join reference sequences: 00:00:00\n",
      "bmax according to bmaxDivN setting: 518300\n",
      "Using parameters --bmax 388725 --dcv 1024\n",
      "  Doing ahead-of-time memory usage test\n",
      "  Passed!  Constructing with these parameters: --bmax 388725 --dcv 1024\n",
      "Constructing suffix-array element generator\n",
      "Building DifferenceCoverSample\n",
      "  Building sPrime\n",
      "  Building sPrimeOrder\n",
      "  V-Sorting samples\n",
      "  V-Sorting samples time: 00:00:00\n",
      "  Allocating rank array\n",
      "  Ranking v-sort output\n",
      "  Ranking v-sort output time: 00:00:00\n",
      "  Invoking Larsson-Sadakane on ranks\n",
      "  Invoking Larsson-Sadakane on ranks time: 00:00:00\n",
      "  Sanity-checking and returning\n",
      "Building samples\n",
      "Reserving space for 12 sample suffixes\n",
      "Generating random suffixes\n",
      "QSorting 12 sample offsets, eliminating duplicates\n",
      "QSorting sample offsets, eliminating duplicates time: 00:00:00\n",
      "Multikey QSorting 12 samples\n",
      "  (Using difference cover)\n",
      "  Multikey QSorting samples time: 00:00:00\n",
      "Calculating bucket sizes\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Split 1, merged 7; iterating...\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Split 1, merged 0; iterating...\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Avg bucket size: 259149 (target: 388724)\n",
      "Converting suffix-array elements to index image\n",
      "Allocating ftab, absorbFtab\n",
      "Entering Ebwt loop\n",
      "Getting block 1 of 8\n",
      "  Reserving size (388725) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 119245\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 119246\n",
      "Getting block 2 of 8\n",
      "  Reserving size (388725) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 378971\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 378972\n",
      "Getting block 3 of 8\n",
      "  Reserving size (388725) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:01\n",
      "  Sorting block of length 267071\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 267072\n",
      "Getting block 4 of 8\n",
      "  Reserving size (388725) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 309983\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 309984\n",
      "Getting block 5 of 8\n",
      "  Reserving size (388725) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 289416\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 289417\n",
      "Getting block 6 of 8\n",
      "  Reserving size (388725) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 293053\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 293054\n",
      "Getting block 7 of 8\n",
      "  Reserving size (388725) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 150814\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 150815\n",
      "Getting block 8 of 8\n",
      "  Reserving size (388725) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 264640\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 264641\n",
      "Exited Ebwt loop\n",
      "fchr[A]: 0\n",
      "fchr[C]: 502716\n",
      "fchr[G]: 1031282\n",
      "fchr[T]: 1599992\n",
      "fchr[$]: 2073200\n",
      "Exiting Ebwt::buildToDisk()\n",
      "Returning from initFromVector\n",
      "Wrote 5104852 bytes to primary EBWT file: batch_1.rev.1.ebwt\n",
      "Wrote 259156 bytes to secondary EBWT file: batch_1.rev.2.ebwt\n",
      "Re-opening _in1 and _in2 as input streams\n",
      "Returning from Ebwt constructor\n",
      "Headers:\n",
      "    len: 2073200\n",
      "    bwtLen: 2073201\n",
      "    sz: 518300\n",
      "    bwtSz: 518301\n",
      "    lineRate: 6\n",
      "    linesPerSide: 1\n",
      "    offRate: 5\n",
      "    offMask: 0xffffffe0\n",
      "    isaRate: -1\n",
      "    isaMask: 0xffffffff\n",
      "    ftabChars: 10\n",
      "    eftabLen: 20\n",
      "    eftabSz: 80\n",
      "    ftabLen: 1048577\n",
      "    ftabSz: 4194308\n",
      "    offsLen: 64788\n",
      "    offsSz: 259152\n",
      "    isaLen: 0\n",
      "    isaSz: 0\n",
      "    lineSz: 64\n",
      "    sideSz: 64\n",
      "    sideBwtSz: 56\n",
      "    sideBwtLen: 224\n",
      "    numSidePairs: 4628\n",
      "    numSides: 9256\n",
      "    numLines: 9256\n",
      "    ebwtTotLen: 592384\n",
      "    ebwtTotSz: 592384\n",
      "    reverse: 0\n",
      "Total time for backward call to driver() for mirror index: 00:00:02\n"
     ]
    }
   ],
   "source": [
    "#create bowtie reference\n",
    "!bowtie-build seqsforBOWTIE.fa batch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mbatch_1.1.ebwt\u001b[0m*  \u001b[01;32mbatch_1.3.ebwt\u001b[0m*  \u001b[01;32mbatch_1.rev.1.ebwt\u001b[0m*  \u001b[34;42mbowtie-1.1.2\u001b[0m/\r\n",
      "\u001b[01;32mbatch_1.2.ebwt\u001b[0m*  \u001b[01;32mbatch_1.4.ebwt\u001b[0m*  \u001b[01;32mbatch_1.rev.2.ebwt\u001b[0m*  \u001b[01;32mseqsforBOWTIE.fa\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# reads processed: 14600\r\n",
      "# reads with at least one reported alignment: 14600 (100.00%)\r\n",
      "# reads that failed to align: 0 (0.00%)\r\n",
      "Reported 14600 alignments to 1 output stream(s)\r\n"
     ]
    }
   ],
   "source": [
    "#align seqs.fasta file against itself\n",
    "!bowtie -f -v 3 --sam --sam-nohead \\\n",
    "batch_1 \\\n",
    "seqsforBOWTIE.fa \\\n",
    "batch_1_BOWTIEout.sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Bowtie output lines read: 14600\n",
      "Number of sequences written to output: 14600\n"
     ]
    }
   ],
   "source": [
    "#filter out the sequences that aligned to sequences other than themselves\n",
    "!python parseBowtie_DD.py ../stacksCombo_m10/BOWTIE/batch_1_BOWTIEout.sam ../stacksCombo_m10/BOWTIE/batch_1_BOWTIEout_filtered.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis/stacksCombo_m10\n"
     ]
    }
   ],
   "source": [
    "cd ../stacksCombo_m10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter with BLAST\n",
    "!mkdir BLAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis/stacksCombo_m10/BLAST\n"
     ]
    }
   ],
   "source": [
    "cd BLAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv ../BOWTIE/batch_1_BOWTIEout_filtered.fa batch_1_BOWTIEout_filtered.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Building a new DB, current time: 11/17/2016 13:49:11\n",
      "New DB name:   /mnt/hgfs/Pacific cod/DataAnalysis/stacksCombo_m10/BLAST/batch_1_BOWTIEfiltered\n",
      "New DB title:  batch_1_BOWTIEout_filtered.fa\n",
      "Sequence type: Nucleotide\n",
      "Keep Linkouts: T\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 14600 sequences in 0.271607 seconds.\n"
     ]
    }
   ],
   "source": [
    "#make a blast database\n",
    "!makeblastdb -in batch_1_BOWTIEout_filtered.fa \\\n",
    "-parse_seqids \\\n",
    "-dbtype nucl \\\n",
    "-out batch_1_BOWTIEfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#blast the fasta file against the database (itself)\n",
    "!blastn -query batch_1_BOWTIEout_filtered.fa \\\n",
    "-db batch_1_BOWTIEfiltered \\\n",
    "-out batch_1_BowtieBlastFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter out loci that aligned to themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis/scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identifying which loci are 'good' and 'bad' based on BLAST alignments...\n",
      "Writing 'good' and 'bad' loci to their respective files...\n"
     ]
    }
   ],
   "source": [
    "!python checkBlastResults_DD.py \\\n",
    "../stacksCombo_m10/BLAST/batch_1_BowtieBlastFiltered \\\n",
    "../stacksCombo_m10/BLAST/batch_1_BOWTIEout_filtered.fa \\\n",
    "../stacksCombo_m10/BLAST/batch_1_BowtieBlastFiltered_GOOD.fa \\\n",
    "../stacksCombo_m10/BLAST/batch_1_BowtieBlastFiltered_BAD.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis/stacksCombo_m10/BLAST\n"
     ]
    }
   ],
   "source": [
    "cd ../stacksCombo_m10/BLAST/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14303\r\n"
     ]
    }
   ],
   "source": [
    "#count number of loci retained\n",
    "!grep \">\" batch_1_BowtieBlastFiltered_GOOD.fa | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\r\n"
     ]
    }
   ],
   "source": [
    "!grep \">\" batch_1_BowtieBlastFiltered_BAD.fa | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Pacific cod/DataAnalysis/stacksCombo_m10\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!gzip batch_1.catalog.tags.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now I have my reference database in a fasta file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** STEP NINE: RUN PSTACKS AGAINST NEW REFERENCE DATABASE **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## STOP: \n",
    "\n",
    "After groundtruthing the .matches files for an individual INCLUDED in the cstacks catalog, the SNPs identified in the file DO NOT match the ACTUAL SNPs from the .tags file's reads. \n",
    "\n",
    "\n",
    "Have to stop here. Will try an earlier AND a later version of stacks this weekend to see if the same problem is still occurring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
